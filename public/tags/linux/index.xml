<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Linux on EETH - Blog</title><link>http://localhost:1313/tags/linux/</link><description>Recent content in Linux on EETH - Blog</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><managingEditor>jayeshjoshi08jj@gmail.com (hkcfs)</managingEditor><webMaster>jayeshjoshi08jj@gmail.com (hkcfs)</webMaster><copyright>hkcfs</copyright><lastBuildDate>Sun, 15 Jun 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/linux/index.xml" rel="self" type="application/rss+xml"/><item><title>The Art of Self Compiling When Your Package Manager Is Not Enough</title><link>http://localhost:1313/blog/the-art-of-self-compiling-when-your-package-manager-isnot-enough/</link><pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate><author>jayeshjoshi08jj@gmail.com (hkcfs)</author><guid>http://localhost:1313/blog/the-art-of-self-compiling-when-your-package-manager-isnot-enough/</guid><description>&lt;p>For most Linux users, our package manager – be it &lt;code>pacman&lt;/code>, &lt;code>apt&lt;/code>, &lt;code>dnf&lt;/code>, or &lt;code>zypper&lt;/code> – is our best friend. It’s the gatekeeper to thousands of applications, effortlessly handling installation, updates, and dependencies. It’s convenient, reliable, and generally, it just &lt;em>works&lt;/em>. But what happens when &amp;ldquo;just works&amp;rdquo; isn&amp;rsquo;t enough? What if you need the absolute latest features, a specific optimization, or a niche piece of software that your distro simply doesn&amp;rsquo;t package?&lt;/p></description><content:encoded><![CDATA[<p>For most Linux users, our package manager – be it <code>pacman</code>, <code>apt</code>, <code>dnf</code>, or <code>zypper</code> – is our best friend. It’s the gatekeeper to thousands of applications, effortlessly handling installation, updates, and dependencies. It’s convenient, reliable, and generally, it just <em>works</em>. But what happens when &ldquo;just works&rdquo; isn&rsquo;t enough? What if you need the absolute latest features, a specific optimization, or a niche piece of software that your distro simply doesn&rsquo;t package?</p>
<p>This is where the venerable art of <strong>self-compiling software from source</strong> comes in. It’s a skill that harkens back to the early days of Linux and remains incredibly powerful, offering a level of control and flexibility that no pre-packaged binary can match.</p>
<h2 id="the-package-managers-convenience-and-its-limits">The Package Manager&rsquo;s Convenience, and Its Limits</h2>
<p>Package managers are designed for mass distribution and stability. They provide tested, stable versions of software, ensuring compatibility within your system. This is fantastic for daily driving.</p>
<p>However, this convenience comes with inherent limitations:</p>
<ul>
<li><strong>Version Lag:</strong> Distro repositories often lag behind upstream releases. If a new version of your favorite tool just dropped with a critical bug fix or a groundbreaking feature, your package manager might not offer it for weeks or even months.</li>
<li><strong>Generic Optimization:</strong> Packages are compiled to run on a wide range of hardware. This means they rarely take advantage of specific CPU features unique to <em>your</em> machine. They are compiled with generic flags, not <code>march=native</code> or <code>mtune=native</code>.</li>
<li><strong>Limited Customization:</strong> You get the software as the package maintainer intended. Want to disable a specific feature, enable an experimental one, or compile with a particular library version? Usually, you can&rsquo;t, without resorting to custom repositories or AUR packages (which are often just automated compilation scripts anyway).</li>
<li><strong>Niche Software:</strong> For cutting-edge research tools, obscure utilities, or software in early development, a package might not exist at all.</li>
</ul>
<h2 id="why-bother-compiling-from-source-the-power-of-control">Why Bother Compiling from Source? The Power of Control</h2>
<p>When you compile software yourself, you gain unparalleled control and unlock several significant advantages:</p>
<ol>
<li><strong>Access to the Bleeding Edge:</strong> Get the absolute latest versions, features, and bug fixes directly from the source repository. This is crucial for developers, early adopters, or anyone working with fast-evolving projects.</li>
<li><strong>Tailored Performance &amp; Optimization:</strong> This is a big one. You can compile software specifically for <em>your</em> CPU architecture. Using flags like <code>-march=native</code>, <code>-mtune=native</code>, and advanced optimization levels (<code>-O3</code>, <code>-Ofast</code>) can result in binaries that run noticeably faster on your system. This is particularly relevant for CPU-intensive applications like video encoders, compilers, or scientific software.</li>
<li><strong>Granular Customization:</strong> You decide what goes into your software. Want to compile <code>Vim</code> with Python 3 support but without Ruby? Done. Want to remove unnecessary dependencies or build with specific hardened flags? You can. This allows you to create highly specialized binaries that perfectly fit your needs and minimize bloat.</li>
<li><strong>Deeper Understanding of the Software Stack:</strong> The act of compiling forces you to confront dependencies, build systems (Makefiles, CMake, Meson), and the intricacies of linking libraries. It&rsquo;s an invaluable learning experience that demystifies how software is actually put together on a Linux system. You&rsquo;ll become a better troubleshooter by understanding the plumbing.</li>
<li><strong>Running Niche or Unpackaged Software:</strong> If a project is new, obscure, or simply not popular enough for your distro&rsquo;s maintainers to package, compiling from source is often your <em>only</em> option to get it running.</li>
<li><strong>Contributing Back:</strong> Understanding the build process is the first step towards submitting bug reports, patches, or even new features to upstream projects.</li>
</ol>
<h2 id="the-downsides-its-not-always-a-smooth-ride">The Downsides: It&rsquo;s Not Always a Smooth Ride</h2>
<p>Compiling from source isn&rsquo;t without its challenges. It requires more effort and attention:</p>
<ul>
<li><strong>Dependency Hell:</strong> Manually resolving all build dependencies can be tedious. You might need to install development libraries (<code>-dev</code> packages) that aren&rsquo;t typically installed on a desktop system.</li>
<li><strong>Time Consuming:</strong> Compiling large projects (like a new kernel or a full desktop environment) can take hours, even on fast machines.</li>
<li><strong>Manual Updates:</strong> Unlike packages, self-compiled software doesn&rsquo;t automatically update. You&rsquo;ll need to manually fetch new source code, recompile, and reinstall to get updates. This can become a maintenance burden.</li>
<li><strong>Potential for Instability:</strong> Self-compiled software might be less tested than official distro packages, potentially leading to unexpected bugs or conflicts with other system components.</li>
<li><strong>No Easy Rollbacks:</strong> If a self-compiled binary breaks your system, uninstalling it and reverting to a previous state is often a manual process, unlike the atomic upgrades/downgrades offered by package managers.</li>
</ul>
<h2 id="getting-started-the-basic-workflow">Getting Started: The Basic Workflow</h2>
<p>Despite the potential pitfalls, the core process of compiling is often surprisingly simple:</p>
<ol>
<li><strong>Install Build Tools:</strong> Ensure you have essential development tools. On Debian/Ubuntu:





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo apt install build-essential git autoconf libtool pkg-config</span></span></code></pre></div>On Fedora:





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo dnf install @development-tools git autoconf libtool pkg-config</span></span></code></pre></div>On Arch Linux:





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo pacman -S base-devel git autoconf libtool pkg-config</span></span></code></pre></div></li>
<li><strong>Get the Source Code:</strong> Use <code>git clone</code> for version-controlled projects, or download a tarball (<code>.tar.gz</code>, <code>.zip</code>) from the project&rsquo;s website.





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">git clone https://github.com/someuser/someproject.git
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="nb">cd</span> someproject</span></span></code></pre></div></li>
<li><strong>Configure:</strong> Most projects use a <code>configure</code> script to prepare the build system based on your environment and desired features. You can often pass flags here to enable/disable features.





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">./configure --prefix<span class="o">=</span>/usr/local --enable-feature-x --disable-feature-y</span></span></code></pre></div>(Check <code>README</code> or <code>INSTALL</code> files for specific configuration options.)</li>
<li><strong>Compile:</strong> This is where the magic happens. The <code>make</code> command starts the compilation process.





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">make -j<span class="k">$(</span>nproc<span class="k">)</span> <span class="c1"># Use -j to parallelize compilation, using all CPU cores</span></span></span></code></pre></div></li>
<li><strong>Install:</strong> This copies the compiled binaries, libraries, and other files to their final destination (often <code>/usr/local/bin</code>, <code>/usr/local/lib</code>, etc.).





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo make install</span></span></code></pre></div><strong>Pro Tip: Use <code>checkinstall</code>!</strong> Instead of <code>sudo make install</code>, consider <code>sudo checkinstall</code>. This utility creates a <code>.deb</code>, <code>.rpm</code>, or <code>.tgz</code> package from your compilation, allowing you to install, manage, and uninstall your self-compiled software with your system&rsquo;s package manager, greatly simplifying maintenance.</li>
</ol>
<h2 id="conclusion-a-powerful-tool-not-a-daily-driver-usually">Conclusion: A Powerful Tool, Not a Daily Driver (Usually)</h2>
<p>Self-compiling software isn&rsquo;t meant to replace your package manager for every application. For the vast majority of software you use, relying on your distro&rsquo;s repositories is the sensible choice for stability and ease of maintenance.</p>
<p>However, when you hit those limits – the urgent need for a new feature, the desire for peak performance, or the necessity to run an unpackaged tool – compiling from source transforms from a chore into a powerful capability. It&rsquo;s a skill that deepens your understanding of Linux and puts you firmly in control of your software.</p>
<p>So, the next time your package manager can&rsquo;t give you exactly what you want, don&rsquo;t despair. Embrace the art of self-compiling. Your system (and your inner tinkerer) might just thank you for it.</p>
<hr>
]]></content:encoded></item><item><title>Podman: The Better Container Engine (Yes, I Said It)</title><link>http://localhost:1313/blog/podman-the-better-container-engine/</link><pubDate>Thu, 22 May 2025 00:00:00 +0000</pubDate><author>jayeshjoshi08jj@gmail.com (hkcfs)</author><guid>http://localhost:1313/blog/podman-the-better-container-engine/</guid><description>&lt;p>For years, &amp;ldquo;Docker&amp;rdquo; has been synonymous with &amp;ldquo;containers.&amp;rdquo; It revolutionized how we build, ship, and run applications, making complex deployments incredibly simple. But just like with init systems or file systems, the Linux world thrives on choice and innovation. While Docker remains a dominant force, a powerful challenger has risen from the depths of the Linux ecosystem: &lt;strong>Podman&lt;/strong>.&lt;/p>
&lt;p>And I&amp;rsquo;m here to tell you why, for many users (especially those deeply entrenched in the Linux world), Podman isn&amp;rsquo;t just an alternative; it&amp;rsquo;s arguably the &lt;strong>better&lt;/strong> container engine.&lt;/p></description><content:encoded><![CDATA[<p>For years, &ldquo;Docker&rdquo; has been synonymous with &ldquo;containers.&rdquo; It revolutionized how we build, ship, and run applications, making complex deployments incredibly simple. But just like with init systems or file systems, the Linux world thrives on choice and innovation. While Docker remains a dominant force, a powerful challenger has risen from the depths of the Linux ecosystem: <strong>Podman</strong>.</p>
<p>And I&rsquo;m here to tell you why, for many users (especially those deeply entrenched in the Linux world), Podman isn&rsquo;t just an alternative; it&rsquo;s arguably the <strong>better</strong> container engine.</p>
<h2 id="the-docker-reign-why-it-took-over">The Docker Reign: Why It Took Over</h2>
<p>Docker&rsquo;s meteoric rise was well-deserved. It brought containerization to the masses, simplified complex dependency management, and provided a user-friendly CLI that abstracted away much of the underlying complexity. Its &ldquo;build once, run anywhere&rdquo; promise resonated deeply with developers and operations teams alike. Docker Desktop for Windows and macOS further solidified its position by offering a seamless experience on non-Linux operating systems.</p>
<p>But Docker isn&rsquo;t without its quirks, and as containerization matured, some of its foundational design choices began to show their age, particularly around its daemon-based architecture and evolving licensing terms.</p>
<h2 id="enter-podman-the-daemonless-champion">Enter Podman: The Daemonless Champion</h2>
<p>Podman (short for <strong>Pod Manager</strong>) emerged from Red Hat, designed from the ground up to be a daemonless container engine. It&rsquo;s built to be compatible with Docker&rsquo;s command-line interface, meaning if you know Docker, you already know Podman. But its core differences make it a compelling choice for the modern Linux user.</p>
<p>Here&rsquo;s why Podman is making a strong case for being the superior container engine:</p>
<h3 id="1-no-daemon-no-root-more-security">1. No Daemon, No Root, More Security</h3>
<p>This is Podman&rsquo;s killer feature. Docker relies on a persistent daemon running in the background (the <code>dockerd</code> process), which typically runs as root. This daemon is a single point of failure and a potential security vulnerability. If someone compromises the Docker daemon, they essentially gain root access to your system.</p>
<p>Podman, on the other hand, is <strong>daemonless</strong>. It runs as a normal process, just like any other command-line tool. When you run <code>podman run</code>, it executes, starts your container, and then exits. This means:</p>
<ul>
<li><strong>No Single Point of Failure:</strong> No daemon to crash or be exploited.</li>
<li><strong>Rootless Containers by Default:</strong> You can (and should) run containers as a non-root user. This is a massive security win. If a process inside your container is compromised, it only has the privileges of your user account, not root. This aligns perfectly with the principle of least privilege.</li>
<li><strong>Simpler Architecture:</strong> Less complexity, fewer moving parts.</li>
</ul>
<h3 id="2-native-pods-kubernetes-friendly">2. Native Pods: Kubernetes-Friendly</h3>
<p>Podman&rsquo;s name isn&rsquo;t just for show. It natively understands and operates on the concept of <strong>pods</strong>, which are fundamental to Kubernetes. A pod is a group of one or more containers sharing resources like network namespaces and storage.</p>
<ul>
<li><strong>Seamless Kubernetes Transition:</strong> Podman makes it incredibly easy to take a <code>pod</code> you&rsquo;ve developed and tested locally and deploy it directly to a Kubernetes cluster. You can use <code>podman generate kube</code> to create Kubernetes YAML from your running pods, streamlining your development-to-production workflow.</li>
<li><strong>Local Multi-Container Management:</strong> Even without Kubernetes, the <code>pod</code> concept allows you to manage related containers as a single unit, which is often how real-world applications are structured.</li>
</ul>
<h3 id="3-systemd-integration-a-linux-native">3. Systemd Integration: A Linux Native</h3>
<p>Because Podman doesn&rsquo;t rely on a daemon, it integrates beautifully with <code>systemd</code>, the default init system for most modern Linux distributions. You can manage your containers directly as <code>systemd</code> services, just like any other application.</p>
<ul>
<li><strong>Robust Service Management:</strong> Leverage <code>systemd</code>&rsquo;s powerful features like dependency management, auto-restarts, resource limits, and logging directly for your containers.</li>
<li><strong>No Daemon Workarounds:</strong> No need for complex <code>systemd</code> unit files to manage a Docker daemon; you&rsquo;re managing the containers themselves.</li>
</ul>
<h3 id="4-open-standards-oci-future-proofing">4. Open Standards (OCI): Future-Proofing</h3>
<p>Podman strictly adheres to Open Container Initiative (OCI) standards for container images and runtimes. This commitment to open standards ensures greater interoperability and avoids vendor lock-in.</p>
<ul>
<li><strong>OCI Image Specification:</strong> Podman uses OCI images, meaning images built with Docker are compatible with Podman, and vice-versa.</li>
<li><strong>OCI Runtime Specification:</strong> Podman uses OCI-compliant runtimes (like runc, crun), ensuring consistency and security.</li>
</ul>
<h3 id="5-familiarity-the-transition-is-easy">5. Familiarity: The Transition is Easy</h3>
<p>One of the biggest hurdles when switching tools is the learning curve. With Podman, this hurdle is practically non-existent. Most Docker commands work identically with Podman.</p>
<ul>
<li><code>docker run</code> becomes <code>podman run</code></li>
<li><code>docker build</code> becomes <code>podman build</code></li>
<li><code>docker images</code> becomes <code>podman images</code></li>
<li>&hellip;you get the idea.</li>
</ul>
<p>This low barrier to entry means you can start experimenting with Podman today without having to relearn your entire container workflow.</p>
<h3 id="6-the-open-source-ethos">6. The Open-Source Ethos</h3>
<p>While Docker has shifted towards a more commercial focus with changes to its Desktop product&rsquo;s licensing, Podman remains fully open-source, maintained by Red Hat and a vibrant community. It&rsquo;s deeply integrated into the Linux ecosystem, making it a natural fit for those who value open standards and community-driven development.</p>
<h2 id="when-docker-still-has-an-edge-the-balance">When Docker Still Has an Edge (The Balance)</h2>
<p>It&rsquo;s important to acknowledge that Docker still holds some advantages, particularly in certain niches:</p>
<ul>
<li><strong>Docker Desktop:</strong> For Windows and macOS users, Docker Desktop provides an incredibly convenient, integrated experience that bundles a Linux VM, Kubernetes, and a GUI. While Podman offers <code>podman-machine</code> for similar functionality on non-Linux, Docker Desktop is still more mature and polished for these specific OSes.</li>
<li><strong>Ecosystem Maturity:</strong> Docker&rsquo;s ecosystem is vast, with a massive number of tutorials, third-party integrations, and community solutions built specifically around it. While Podman is catching up rapidly, some niche tools might still assume a Docker environment.</li>
</ul>
<h2 id="conclusion-make-the-switch-or-try-it">Conclusion: Make the Switch (or Try It!)</h2>
<p>For Linux users, especially those concerned with security, system integration, and open standards, Podman is a highly compelling choice. Its daemonless, rootless architecture is a significant step forward in container security and simplifies system management. Its native support for pods and seamless integration with <code>systemd</code> make it a truly &ldquo;Linux native&rdquo; container engine.</p>
<p>If you&rsquo;re still using Docker on Linux, I highly encourage you to give Podman a try. The transition is minimal, the benefits are substantial, and you might just find yourself saying, &ldquo;Yes, Podman is better.&rdquo;</p>
<p>Take back control, enhance your security, and embrace the future of Linux containerization.</p>
]]></content:encoded></item><item><title>Self-Hosting: Take Back Your Data (and Your Sanity?)</title><link>http://localhost:1313/blog/self-hosting-take-back-your-data/</link><pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate><author>jayeshjoshi08jj@gmail.com (hkcfs)</author><guid>http://localhost:1313/blog/self-hosting-take-back-your-data/</guid><description>&lt;p>We live in the age of cloud services. Need to store files? Google Drive, Dropbox. Want to watch movies? Netflix, Prime Video. Listen to music? Spotify, Apple Music. It&amp;rsquo;s all there, readily available, just a click away. Convenient? Absolutely. But have you ever stopped to think about where your data actually &lt;em>is&lt;/em>? Spoiler alert: it&amp;rsquo;s not really &lt;em>your&amp;rsquo;s&lt;/em>. It&amp;rsquo;s sitting on someone else&amp;rsquo;s servers, governed by their terms of service, and potentially vulnerable in ways you might not even imagine.&lt;/p></description><content:encoded><![CDATA[<p>We live in the age of cloud services. Need to store files? Google Drive, Dropbox. Want to watch movies? Netflix, Prime Video. Listen to music? Spotify, Apple Music. It&rsquo;s all there, readily available, just a click away. Convenient? Absolutely. But have you ever stopped to think about where your data actually <em>is</em>? Spoiler alert: it&rsquo;s not really <em>your&rsquo;s</em>. It&rsquo;s sitting on someone else&rsquo;s servers, governed by their terms of service, and potentially vulnerable in ways you might not even imagine.</p>
<p>This is where <strong>self-hosting</strong> comes in. The idea is simple: instead of relying on big tech companies for everything, you host the services you use yourself, on your own hardware, under your own control.</p>
<h2 id="why-bother-self-hosting">Why Bother Self-Hosting?</h2>
<p>You might be thinking, &ldquo;Why would I make my life harder? The cloud works perfectly fine!&rdquo; And you&rsquo;re not wrong. Cloud services are slick, user-friendly, and often &ldquo;just work.&rdquo; But there are compelling reasons to consider the self-hosted path:</p>
<ul>
<li><strong>Privacy and Control:</strong> This is the big one. When you self-host, <em>you</em> control your data. No more worrying about privacy policies changing overnight, algorithms scanning your files, or your data being used to train some AI you never asked for. It&rsquo;s your server, your rules.</li>
<li><strong>Learning and Customization:</strong> Setting up and maintaining your own services is a fantastic learning experience. You&rsquo;ll dive into Linux, networking, security, and a whole bunch of other cool tech stuff. Plus, you get to customize everything exactly how you want it. Tired of the limitations of a cloud service? Self-hosting lets you tweak, modify, and extend to your heart&rsquo;s content.</li>
<li><strong>Potentially Lower Cost (Long Term):</strong> Cloud subscriptions can add up over time. If you&rsquo;re tech-savvy and have some spare hardware lying around (or are willing to invest in a small server), self-hosting can be cheaper in the long run. Think about it: one upfront cost for hardware vs. recurring monthly fees for years.</li>
</ul>
<h2 id="its-not-all-sunshine-and-roses-the-challenges">It&rsquo;s Not All Sunshine and Roses (The Challenges)</h2>
<p>Let&rsquo;s be real, self-hosting isn&rsquo;t for everyone. It comes with its own set of challenges:</p>
<ul>
<li><strong>Technical Complexity:</strong> Setting up and managing servers requires technical skills. You&rsquo;ll need to be comfortable with the command line, configuration files, and troubleshooting when things go wrong (and they <em>will</em> go wrong, eventually).</li>
<li><strong>Maintenance and Security:</strong> You become your own sysadmin. This means you&rsquo;re responsible for updates, security patches, backups, and ensuring your services are running smoothly. Security is paramount – a misconfigured server can expose your data to the internet.</li>
<li><strong>Initial Setup Time:</strong> Don&rsquo;t expect to have a fully functional self-hosted setup in an afternoon. It takes time, effort, and patience to set everything up correctly.</li>
</ul>
<h2 id="what-can-you-actually-self-host">What Can You Actually Self-Host?</h2>
<p>The possibilities are vast! Here are a few popular examples to get you started:</p>
<ul>
<li><strong>Nextcloud:</strong> Your own personal cloud storage and collaboration platform. Think Google Drive/Dropbox, but you control the server. File storage, calendars, contacts, photo galleries, and much more.</li>
<li><strong>Jellyfin:</strong> A free and open-source media server. Stream your movies, TV shows, and music to any device in your home (and even outside). A great alternative to Plex or Emby.</li>
<li><strong>Vaultwarden (Bitwarden-compatible):</strong> A lightweight and open-source password manager. Keep your passwords secure and accessible across all your devices. Why trust your passwords to a third-party when you can host your own?</li>
<li><strong>SearXNG:</strong> A privacy-respecting metasearch engine. Aggregates results from various search engines without tracking you. Take back your search privacy from Google and others.</li>
</ul>
<h2 id="tools-of-the-trade">Tools of the Trade</h2>
<p>To make self-hosting easier, there are some fantastic tools available:</p>
<ul>
<li><strong>Docker/Podman:</strong> Containerization is your best friend. These tools allow you to run services in isolated containers, simplifying setup, updates, and management.</li>
<li><strong>Reverse Proxies (Nginx/Caddy):</strong> Essential for routing traffic to different services running on your server and handling SSL/TLS encryption (for secure HTTPS connections).</li>
<li><strong>Dynamic DNS:</strong> If you have a dynamic IP address from your ISP (most home users do), Dynamic DNS services keep your domain name pointed to your current IP address, even when it changes.</li>
</ul>
<h2 id="is-self-hosting-right-for-you">Is Self-Hosting Right for You?</h2>
<p>Self-hosting isn&rsquo;t a magic bullet. It requires effort and technical know-how. But if you value privacy, control, and learning, it&rsquo;s an incredibly rewarding path to explore. It&rsquo;s about taking back ownership of your digital life, one service at a time.</p>
<p>Maybe you start small, with a password manager or a file storage solution. Or maybe you dive in headfirst and build your own self-hosted empire. The choice is yours. Just remember, with great power comes great responsibility&hellip; and maybe a few late nights troubleshooting server issues. But hey, that&rsquo;s part of the fun, right?</p>
]]></content:encoded></item><item><title>Beyond Systemd Exploring Linux Init Systems</title><link>http://localhost:1313/blog/beyond-systemd-exploring-linux-init-systems/</link><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><author>jayeshjoshi08jj@gmail.com (hkcfs)</author><guid>http://localhost:1313/blog/beyond-systemd-exploring-linux-init-systems/</guid><description>&lt;p>Systemd. Just the name can spark heated debates in the Linux community. Love it or hate it, systemd is undeniably the dominant init system in modern Linux distributions. It&amp;rsquo;s become so ubiquitous that for many, &amp;ldquo;init system&amp;rdquo; and &amp;ldquo;systemd&amp;rdquo; are practically synonymous.&lt;/p>
&lt;p>But Linux, in its beautiful, chaotic glory, is all about choice. And when it comes to init systems, systemd is far from the only game in town. While systemd aims to be an &amp;ldquo;everything-but-the-kitchen-sink&amp;rdquo; system and service manager, there&amp;rsquo;s a whole ecosystem of alternative init systems that take different approaches, often prioritizing simplicity, speed, and adherence to the Unix philosophy.&lt;/p></description><content:encoded><![CDATA[<p>Systemd. Just the name can spark heated debates in the Linux community. Love it or hate it, systemd is undeniably the dominant init system in modern Linux distributions. It&rsquo;s become so ubiquitous that for many, &ldquo;init system&rdquo; and &ldquo;systemd&rdquo; are practically synonymous.</p>
<p>But Linux, in its beautiful, chaotic glory, is all about choice. And when it comes to init systems, systemd is far from the only game in town. While systemd aims to be an &ldquo;everything-but-the-kitchen-sink&rdquo; system and service manager, there&rsquo;s a whole ecosystem of alternative init systems that take different approaches, often prioritizing simplicity, speed, and adherence to the Unix philosophy.</p>
<p>So, let&rsquo;s step outside the systemd bubble and explore some of these fascinating alternatives. Why might you consider looking beyond systemd? And what options are out there?</p>
<h2 id="why-venture-beyond-systemd-its-not-just-about-init-freedom">Why Venture Beyond Systemd? (It&rsquo;s Not Just About &ldquo;Init Freedom&rdquo;)</h2>
<p>Before we dive into specific init systems, let&rsquo;s address the &ldquo;why.&rdquo; Why would someone willingly choose something <em>other</em> than the default, widely supported, and feature-rich systemd? It&rsquo;s not just about being contrarian or &ldquo;init freedom&rdquo; for the sake of it. There are valid technical and philosophical reasons:</p>
<ul>
<li>
<p><strong>Simplicity and Minimalism:</strong> Systemd, by design, is complex. It&rsquo;s a vast collection of components handling init, service management, logging, timers, network configuration, and much more. For some, this complexity is overkill. Alternative init systems often focus on doing <em>one thing well</em> – process initialization and supervision – and leaving other tasks to dedicated tools. This simplicity can lead to easier configuration, debugging, and a better understanding of the system&rsquo;s inner workings.</p>
</li>
<li>
<p><strong>Performance and Resource Usage:</strong> While systemd is generally performant, its extensive feature set inevitably comes with a resource footprint. Simpler init systems, especially those written in C, can be significantly lighter on resources, both in terms of CPU and memory usage. This can be particularly relevant on resource-constrained systems like embedded devices, older hardware, or even just for those who value a lean and efficient base system.</p>
</li>
<li>
<p><strong>Adherence to Unix Philosophy:</strong> The Unix philosophy emphasizes small, focused tools that do one thing well and can be composed together. Systemd, with its monolithic approach, deviates from this philosophy. Alternative init systems often embrace the Unix way, promoting modularity and composability. This aligns with a certain philosophy of system design and management that many Linux users appreciate.</p>
</li>
<li>
<p><strong>Learning and Deeper Understanding:</strong> Exploring alternative init systems is a fantastic way to deepen your understanding of how a Linux system boots and operates at a fundamental level. Systemd, while powerful, can sometimes feel like a black box. Experimenting with simpler init systems can demystify the boot process and give you a more hands-on, intimate understanding of your system.</p>
</li>
<li>
<p><strong>Just Because You Can (and Want To!):</strong> Let&rsquo;s be honest, part of the fun of Linux is the freedom to tinker and customize. If you&rsquo;re curious about init systems, want to try something different, or simply want to see if there&rsquo;s a &ldquo;better&rdquo; fit for your needs, then exploring alternatives is a perfectly valid reason in itself.</p>
</li>
</ul>
<h2 id="meet-the-contenders-a-glimpse-beyond-systemd">Meet the Contenders: A Glimpse Beyond Systemd</h2>
<p>Now, let&rsquo;s introduce some of the key players in the alternative Linux init system scene. This is not an exhaustive list, but it covers some of the most prominent and interesting options:</p>
<ul>
<li>
<p><strong>sinit (suckless init): The Minimalist King.</strong> As the name suggests, sinit is the init system from the <a href="https://suckless.org/">suckless.org</a> project. It embodies extreme minimalism. sinit is incredibly small, written in C, and focused solely on the core task of process initialization and supervision. It&rsquo;s designed to be fast, secure, and auditable. Configuration is done through C code, which might sound intimidating, but it results in a very tightly controlled and understandable system. If you value extreme simplicity and performance above all else, sinit is worth exploring. Think of it as the &ldquo;bare metal&rdquo; of init systems.</p>
</li>
<li>
<p><strong>dinit: Fast, Feature-Rich Simplicity.</strong> dinit, also written in C, aims for a balance between simplicity and usability. It&rsquo;s faster than systemd in many benchmarks, but still provides a good set of features for service management, dependency handling, and process supervision. dinit uses a relatively straightforward configuration file format and is designed to be easy to understand and use. It&rsquo;s often considered a good &ldquo;middle ground&rdquo; for those seeking an alternative to systemd without sacrificing too much functionality. As mentioned in the Artix blog post, dinit is known for its speed.</p>
</li>
<li>
<p><strong>runit: Process Supervision Powerhouse.</strong> runit is another popular alternative, written in C. It&rsquo;s known for its robust process supervision capabilities. runit focuses heavily on ensuring services are always running and restarting them quickly if they crash. Configuration is done through shell scripts, making it flexible and scriptable. runit is often favored in container environments and minimalist systems where reliable service supervision is paramount.</p>
</li>
<li>
<p><strong>OpenRC: The Modernized SysVinit.</strong> OpenRC is a dependency-based init system that aims to be compatible with traditional SysVinit scripts while adding modern features like parallel startup and dependency management. It&rsquo;s written in C and shell scripts and is the default init system in distributions like Gentoo and Artix (with OpenRC profile). OpenRC provides a more familiar, SysVinit-like feel for those who prefer that style of system management, but with significant improvements in boot speed and service management.</p>
</li>
<li>
<p><strong>initd/SysVinit (Historical Context): The Grandfather (Mostly Retired).</strong> SysVinit (System V init) is the &ldquo;classic&rdquo; init system that was the standard in Linux for a long time. It&rsquo;s based on shell scripts and a sequential boot process. While still functional, SysVinit is generally considered outdated and slow compared to modern alternatives. It lacks features like dependency-based startup and efficient service supervision. Most distributions have moved away from SysVinit in favor of systemd or other modern options. However, understanding SysVinit provides valuable historical context and helps appreciate the evolution of init systems.</p>
</li>
</ul>
<h2 id="comparing-the-init-systems-a-quick-overview">Comparing the Init Systems: A Quick Overview</h2>
<p>To get a clearer picture, here&rsquo;s a simplified comparison table highlighting some key aspects of these init systems:</p>
<table>
  <thead>
      <tr>
          <th>Init System</th>
          <th>Language</th>
          <th>Philosophy</th>
          <th>Complexity</th>
          <th>Speed</th>
          <th>Key Features</th>
          <th>Use Cases</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>systemd</strong></td>
          <td>C</td>
          <td>Monolithic, All-in-one</td>
          <td>High</td>
          <td>Good</td>
          <td>Extensive features, service management, logging, timers, etc.</td>
          <td>Most modern Linux distributions, general-purpose systems</td>
      </tr>
      <tr>
          <td><strong>sinit</strong></td>
          <td>C</td>
          <td>Minimalist, Suckless</td>
          <td>Very Low</td>
          <td>Very High</td>
          <td>Extremely simple, process supervision only</td>
          <td>Embedded systems, minimal systems, performance-critical</td>
      </tr>
      <tr>
          <td><strong>dinit</strong></td>
          <td>C</td>
          <td>Simple, Fast</td>
          <td>Low</td>
          <td>High</td>
          <td>Service management, dependency handling, process supervision</td>
          <td>General-purpose systems, servers, performance-conscious</td>
      </tr>
      <tr>
          <td><strong>runit</strong></td>
          <td>C</td>
          <td>Process Supervision</td>
          <td>Low</td>
          <td>Good</td>
          <td>Robust process supervision, scriptable config</td>
          <td>Containers, minimalist systems, reliable services</td>
      </tr>
      <tr>
          <td><strong>OpenRC</strong></td>
          <td>C/Shell</td>
          <td>SysVinit-like, Modern</td>
          <td>Medium</td>
          <td>Medium</td>
          <td>Dependency-based startup, SysVinit compatibility</td>
          <td>Gentoo, Artix (OpenRC), users preferring SysVinit style</td>
      </tr>
      <tr>
          <td><strong>SysVinit</strong></td>
          <td>Shell</td>
          <td>Traditional, Sequential</td>
          <td>Low</td>
          <td>Low</td>
          <td>Basic process initialization</td>
          <td>Legacy systems, learning historical context</td>
      </tr>
  </tbody>
</table>
<p><strong>Note:</strong> This is a highly simplified comparison. &ldquo;Speed&rdquo; and &ldquo;Complexity&rdquo; are relative terms and depend on specific workloads and configurations.</p>
<h2 id="trying-out-alternatives-taking-the-plunge-carefully">Trying Out Alternatives: Taking the Plunge (Carefully!)</h2>
<p>If you&rsquo;re intrigued and want to experiment with alternative init systems, the easiest way is often to use a distribution that supports them out of the box. Artix Linux, as mentioned earlier, is a great example. It offers a choice of init systems during installation, including sinit, dinit, runit, OpenRC, and of course, systemd. Distributions like Gentoo also provide flexibility in choosing init systems.</p>
<p><strong>Important:</strong> Switching init systems is a fundamental system change and can potentially lead to boot failures if not done correctly. <strong>Always back up your system before making such changes.</strong> Read the documentation for your chosen distribution and init system carefully. Start in a virtual machine if you&rsquo;re unsure.</p>
<p>Here&rsquo;s a very general outline of how you might switch init systems on a distribution like Artix (this is a <em>simplified example</em> and specific steps will vary):</p>
<ol>
<li><strong>Install Artix Linux:</strong> Choose the ISO image with your desired init system (e.g., Artix-dinit, Artix-runit).</li>
<li><strong>During Installation:</strong> Select the appropriate init system profile.</li>
<li><strong>Post-Installation (if switching later):</strong> This is more complex and distribution-specific. It usually involves:</li>
</ol>
<ul>
<li>Installing the new init system packages.</li>
<li>Disabling the current init system&rsquo;s services.</li>
<li>Enabling the new init system&rsquo;s services.</li>
<li>Updating the bootloader configuration to use the new init system.</li>
<li><strong>Reboot and pray (and troubleshoot if needed!).</strong></li>
</ul>
<p><strong>Seriously, back up your system!</strong></p>
<h2 id="conclusion-embrace-the-choice-understand-your-system">Conclusion: Embrace the Choice, Understand Your System</h2>
<p>The world of Linux init systems is richer and more diverse than just systemd. While systemd is powerful and feature-rich, alternatives like sinit, dinit, runit, and OpenRC offer different trade-offs and cater to different needs and philosophies.</p>
<p>There&rsquo;s no single &ldquo;best&rdquo; init system. The &ldquo;best&rdquo; choice depends entirely on your priorities: simplicity, speed, features, Unix philosophy, learning, or simply wanting to explore.</p>
<p>Experimenting with alternative init systems is a valuable learning experience. It forces you to understand the fundamental boot process of your Linux system and appreciate the different ways it can be managed. Whether you stick with systemd or find a new favorite, the journey of exploration is what truly matters.</p>
<p>So, dare to venture beyond systemd. Dive into the documentation, try out a different init system in a VM, and see what you discover. You might be surprised at the options and perspectives that await you in the diverse landscape of Linux init systems.</p>
]]></content:encoded></item><item><title>Cheat.sh vs Tldr Command Line Cheat Sheets Compared</title><link>http://localhost:1313/blog/cheat.sh-vs-tldr-command-line-cheat-sheets-compared/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><author>jayeshjoshi08jj@gmail.com (hkcfs)</author><guid>http://localhost:1313/blog/cheat.sh-vs-tldr-command-line-cheat-sheets-compared/</guid><description>&lt;p>In the world of the command line, efficiency is king. We&amp;rsquo;ve already discussed &lt;code>tldr&lt;/code> as a fantastic tool for quickly accessing simplified man page examples. But &lt;code>tldr&lt;/code> isn&amp;rsquo;t the only cheat sheet game in town. Enter &lt;strong>&lt;code>cheat.sh&lt;/code> (or &lt;code>cht.sh&lt;/code>)&lt;/strong>, another powerful contender for your command-line quick-reference needs.&lt;/p>
&lt;p>Both &lt;code>cheat.sh&lt;/code> and &lt;code>tldr&lt;/code> aim to solve the same problem: making command-line documentation more accessible and less time-consuming than traditional man pages. But they approach this goal in different ways, with distinct features and philosophies. So, which cheat sheet champion deserves a place in your terminal toolkit? Let&amp;rsquo;s dive into a head-to-head comparison.&lt;/p></description><content:encoded><![CDATA[<p>In the world of the command line, efficiency is king. We&rsquo;ve already discussed <code>tldr</code> as a fantastic tool for quickly accessing simplified man page examples. But <code>tldr</code> isn&rsquo;t the only cheat sheet game in town. Enter <strong><code>cheat.sh</code> (or <code>cht.sh</code>)</strong>, another powerful contender for your command-line quick-reference needs.</p>
<p>Both <code>cheat.sh</code> and <code>tldr</code> aim to solve the same problem: making command-line documentation more accessible and less time-consuming than traditional man pages. But they approach this goal in different ways, with distinct features and philosophies. So, which cheat sheet champion deserves a place in your terminal toolkit? Let&rsquo;s dive into a head-to-head comparison.</p>
<h2 id="tldr-concise-community-examples-the-pocket-guide">tldr: Concise Community Examples (The Pocket Guide)</h2>
<p><code>tldr</code> is all about <strong>simplicity and community</strong>. It&rsquo;s a collection of concise, community-maintained cheat sheets focused on the most common use cases for command-line tools.</p>
<p><strong>Key features of <code>tldr</code>:</strong></p>
<ul>
<li><strong>Concise and Focused:</strong> <code>tldr</code> pages are intentionally short and to the point. They prioritize clarity and brevity, giving you just the essential examples you need.</li>
<li><strong>Community-Driven:</strong> The content is created and maintained by a community of contributors, ensuring a wide range of commands are covered and kept up-to-date.</li>
<li><strong>Client-Side Application:</strong> <code>tldr</code> is typically used via a client application installed on your system. This client fetches the cheat sheets and displays them in your terminal.</li>
<li><strong>Focus on Common Use Cases:</strong> <code>tldr</code> pages primarily showcase the most frequent and practical applications of commands.</li>
<li><strong>Multiple Clients:</strong> Available in various languages (Python, Node.js, Go, etc.), offering flexibility in client choice.</li>
</ul>
<p><strong>Pros of <code>tldr</code>:</strong></p>
<ul>
<li><strong>Extremely Easy to Use:</strong> Installation and usage are very simple.</li>
<li><strong>Highly Readable Output:</strong> Cleanly formatted and easy to scan in the terminal.</li>
<li><strong>Great for Beginners:</strong> Less overwhelming than man pages or more complex cheat sheets.</li>
<li><strong>Fast for Quick Lookups:</strong> Ideal when you just need a fast reminder.</li>
</ul>
<p><strong>Cons of <code>tldr</code>:</strong></p>
<ul>
<li><strong>Limited Scope:</strong> By design, <code>tldr</code> pages are not exhaustive. They don&rsquo;t cover every option or edge case.</li>
<li><strong>Less Detail:</strong> Conciseness means less in-depth explanation. For complex commands or nuanced usage, you might still need to consult man pages.</li>
<li><strong>Internet Dependency (Initial Fetch):</strong> While <code>tldr</code> clients often cache pages, the initial fetch and updates require an internet connection.</li>
</ul>
<h3 id="how-to-use-tldr">How to Use <code>tldr</code></h3>
<ol>
<li>
<p><strong>Installation:</strong>  Install the <code>tldr</code> client for your operating system. Common methods include:</p>
<ul>
<li><strong>Using <code>npm</code> (Node.js):</strong>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">npm install -g tldr</span></span></code></pre></div></li>
<li><strong>Using <code>pip</code> (Python):</strong>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">pip install tldr</span></span></code></pre></div></li>
<li><strong>Using your distribution&rsquo;s package manager (e.g., <code>pacman</code>, <code>apt</code>, <code>dnf</code>):</strong>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo pacman -S tldr  <span class="c1"># Arch Linux</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl">sudo apt install tldr  <span class="c1"># Debian/Ubuntu</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl">sudo dnf install tldr  <span class="c1"># Fedora/CentOS</span></span></span></code></pre></div></li>
</ul>
</li>
<li>
<p><strong>Basic Usage:</strong> To get a <code>tldr</code> cheat sheet for a command, simply type <code>tldr</code> followed by the command name:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">tldr command</span></span></code></pre></div><p>For example, to see the <code>tldr</code> page for <code>tar</code>:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">tldr tar</span></span></code></pre></div></li>
<li>
<p><strong>Navigating Pages:</strong>  Use your terminal&rsquo;s scroll keys (usually <code>Shift + Page Up/Down</code> or scroll wheel) to navigate longer <code>tldr</code> pages.</p>
</li>
<li>
<p><strong>Updating Pages:</strong>  To update the cached <code>tldr</code> pages (if your client supports it):</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">tldr --update</span></span></code></pre></div><p>or check your client&rsquo;s specific update command.</p>
</li>
</ol>
<h2 id="cheatsh-chtsh-the-feature-rich-web-powered-encyclopedia-the-deep-dive">cheat.sh (cht.sh): The Feature-Rich, Web-Powered Encyclopedia (The Deep Dive)</h2>
<p><code>cheat.sh</code> takes a different approach. It&rsquo;s more like a <strong>web-powered, feature-rich command-line encyclopedia</strong>. Instead of a client-side application with pre-packaged cheat sheets, <code>cheat.sh</code> is primarily accessed via <code>curl</code> or <code>nc</code> directly from your terminal, fetching information from its online service in real-time.</p>
<p><strong>Key features of <code>cheat.sh</code>:</strong></p>
<ul>
<li><strong>Web-Based Service:</strong> <code>cheat.sh</code> lives online and is accessed via simple command-line tools like <code>curl</code> or <code>netcat</code>.</li>
<li><strong>Extensive Coverage:</strong> <code>cheat.sh</code> boasts an incredibly broad range of topics, going far beyond just basic command-line tools. It covers programming languages, configuration files, DevOps tools, databases, and much more.</li>
<li><strong>Detailed Examples:</strong> Often provides more in-depth examples and explanations compared to <code>tldr</code>.</li>
<li><strong>Language-Specific Cheatsheets:</strong> Excellent support for programming languages, providing code snippets and syntax examples.</li>
<li><strong>Editor Integration:</strong> Can be integrated into various text editors and IDEs for in-context help.</li>
<li><strong>Special Queries:</strong> Supports special queries like <code>apropos</code>, <code>man</code>, <code>info</code>, and even stackoverflow searches directly from the command line.</li>
<li><strong>Customization and Personalization:</strong> Allows for some level of customization and even creating personal cheat sheets.</li>
</ul>
<p><strong>Pros of <code>cheat.sh</code>:</strong></p>
<ul>
<li><strong>Vast Knowledge Base:</strong> Covers a much wider range of topics than <code>tldr</code>.</li>
<li><strong>More Detailed Information:</strong> Examples are often more comprehensive and explanatory.</li>
<li><strong>Language Support is a Major Strength:</strong> Invaluable for programmers.</li>
<li><strong>Web-Based Accessibility:</strong> Works on any system with <code>curl</code> or <code>nc</code>, no client installation needed.</li>
<li><strong>Powerful Search Capabilities:</strong> Easy to search for specific commands or topics.</li>
</ul>
<p><strong>Cons of <code>cheat.sh</code>:</strong></p>
<ul>
<li><strong>Output Can Be Noisy:</strong> The output, while informative, can sometimes be less cleanly formatted than <code>tldr</code>, especially in the terminal.</li>
<li><strong>Real-time Web Dependency:</strong> Requires a constant internet connection to function. No offline caching by default (though some caching mechanisms exist).</li>
<li><strong>Slightly Steeper Learning Curve (Initially):</strong> While basic usage is simple, exploring all its features takes a bit more effort.</li>
<li><strong>Less &ldquo;Pocket Guide,&rdquo; More &ldquo;Online Encyclopedia&rdquo;:</strong> Can feel less focused than <code>tldr</code> for simple command lookups.</li>
</ul>
<h3 id="how-to-use-cheatsh-chtsh">How to Use <code>cheat.sh</code> (cht.sh)</h3>
<p>Using <code>cheat.sh</code> is incredibly simple directly from your terminal, without needing to install a dedicated client:</p>
<ol>
<li>
<p><strong>Basic Usage with <code>curl</code>:</strong>  To get cheat sheets, use <code>curl</code> followed by <code>cht.sh/command</code>.  Replace <code>command</code> with the tool you need help with.</p>
<ul>
<li>For example, to get the <code>tar</code> cheat sheet:





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">curl cht.sh/tar</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p><strong>Basic Usage with <code>netcat</code> (nc):</strong> If <code>curl</code> is not available, you can use <code>netcat</code> ( <code>nc</code> ):</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">nc cht.sh <span class="m">80</span> <span class="o">&lt;&lt;&lt;</span><span class="s1">$&#39;GET /tar HTTP/1.0\nHost: cht.sh&#39;</span></span></span></code></pre></div></li>
<li>
<p><strong>Specifying Options or Subcommands:</strong>  Append options or subcommands to the URL-like path to refine your search:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">curl cht.sh/tar/create  <span class="c1"># tar specific to creating archives</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl">curl cht.sh/python/list comprehensions <span class="c1"># Python list comprehensions</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl">curl cht.sh/go/range <span class="c1"># Go range keyword</span></span></span></code></pre></div></li>
<li>
<p><strong>Language-Specific Help:</strong>  Prefix your query with a programming language name to get language-specific cheat sheets:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">curl cht.sh/python tar  <span class="c1"># Python examples related to tar</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl">curl cht.sh/js array <span class="c1"># JavaScript array methods</span></span></span></code></pre></div></li>
<li>
<p><strong>Special Queries:</strong>  <code>cheat.sh</code> supports special queries:</p>
<ul>
<li><strong><code>man:</code> for man pages:</strong>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">curl cht.sh/man:ls  <span class="c1"># Get the man page for `ls`</span></span></span></code></pre></div></li>
<li><strong><code>info:</code> for info pages:</strong>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">curl cht.sh/info:grep <span class="c1"># Get the info page for `grep`</span></span></span></code></pre></div></li>
<li><strong><code>apropos:</code> for searching commands by keyword:</strong>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">curl cht.sh/apropos:disk space <span class="c1"># Search commands related to disk space</span></span></span></code></pre></div></li>
<li><strong><code>stackoverflow:</code> for Stack Overflow searches:</strong>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">curl cht.sh/stackoverflow:bash loop through files <span class="c1"># Search Stack Overflow for &#34;bash loop through files&#34;</span></span></span></code></pre></div></li>
</ul>
</li>
<li>
<p><strong>Listing Available Cheat Sheets:</strong> To list all available cheat sheets (the list is very long!):</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">curl cht.sh/:list</span></span></code></pre></div></li>
<li>
<p><strong>Getting Help for <code>cheat.sh</code> itself:</strong></p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">curl cht.sh/:help</span></span></code></pre></div></li>
</ol>
<h2 id="tldr-vs-cheatsh-side-by-side-comparison">tldr vs cheat.sh: Side-by-Side Comparison</h2>
<table>
  <thead>
      <tr>
          <th>Feature</th>
          <th><code>tldr</code></th>
          <th><code>cheat.sh</code> (cht.sh)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Data Source</td>
          <td>Community-maintained, curated pages</td>
          <td>Web-based service, dynamically generated</td>
      </tr>
      <tr>
          <td>Content Style</td>
          <td>Concise examples, focused use cases</td>
          <td>More detailed examples, broader scope</td>
      </tr>
      <tr>
          <td>Accessibility</td>
          <td>Client application (terminal)</td>
          <td>Web-based (curl/nc), editor integration</td>
      </tr>
      <tr>
          <td>Scope</td>
          <td>Primarily command-line tools</td>
          <td>Command-line tools, programming, DevOps, etc.</td>
      </tr>
      <tr>
          <td>Offline Access</td>
          <td>Client often caches pages (partial offline)</td>
          <td>Requires internet connection (mostly online)</td>
      </tr>
      <tr>
          <td>Customization</td>
          <td>Limited client-side themes</td>
          <td>More extensive, personal cheat sheets</td>
      </tr>
      <tr>
          <td>Learning Curve</td>
          <td>Very low</td>
          <td>Slightly higher (for advanced features)</td>
      </tr>
      <tr>
          <td>Target Audience</td>
          <td>Beginners, users needing quick reminders</td>
          <td>Developers, system admins, power users</td>
      </tr>
  </tbody>
</table>
<h2 id="when-to-use-which-choosing-your-cheat-sheet-weapon">When to Use Which: Choosing Your Cheat Sheet Weapon</h2>
<p>So, which tool should you choose? It really depends on your needs and preferences:</p>
<ul>
<li>
<p><strong>Choose <code>tldr</code> if:</strong></p>
<ul>
<li>You want a <strong>super simple, fast, and clean</strong> cheat sheet for common command-line tools.</li>
<li>You are a <strong>beginner</strong> or just need quick reminders for basic commands.</li>
<li>You prefer a <strong>client-side application</strong> and like the idea of community-curated content.</li>
<li>You value <strong>offline access</strong> to cheat sheets (after initial download).</li>
</ul>
</li>
<li>
<p><strong>Choose <code>cheat.sh</code> (cht.sh) if:</strong></p>
<ul>
<li>You need a <strong>vast and comprehensive</strong> cheat sheet resource covering a wide range of topics beyond just basic commands.</li>
<li>You are a <strong>developer</strong> and need cheat sheets for programming languages, frameworks, and tools.</li>
<li>You prefer a <strong>web-based, always up-to-date</strong> resource accessible from any terminal with <code>curl</code> or <code>nc</code>.</li>
<li>You want <strong>more detailed examples</strong> and explanations.</li>
<li>You want <strong>advanced features</strong> like language-specific help, editor integration, and special queries.</li>
</ul>
</li>
</ul>
<p><strong>Personal Recommendation:</strong></p>
<p>For many users, <strong><code>tldr</code> is a great starting point</strong> for its simplicity and ease of use. It&rsquo;s perfect for quickly looking up common command examples. As you become more advanced or need a wider range of information, <strong><code>cheat.sh</code> becomes incredibly powerful</strong>, especially for developers and system administrators.</p>
<p><strong>Many power users actually use <em>both</em> <code>tldr</code> and <code>cheat.sh</code></strong>, leveraging <code>tldr</code> for quick, common lookups and <code>cheat.sh</code> for more in-depth information and broader topic coverage.</p>
<p>Ultimately, the best cheat sheet is the one that fits <em>your</em> workflow and helps you be more efficient at the command line. Experiment with both <code>tldr</code> and <code>cheat.sh</code>, and see which one (or both!) becomes your preferred command-line companion.</p>
]]></content:encoded></item><item><title>Artix Refind Clear Booster - most optmized setup</title><link>http://localhost:1313/blog/artix-refind-clear-booster/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><author>jayeshjoshi08jj@gmail.com (hkcfs)</author><guid>http://localhost:1313/blog/artix-refind-clear-booster/</guid><description>&lt;h2 id="arch-linux--btw">Arch Linux – BTW&lt;/h2>
&lt;p>AAA, one of the most loved (cough, cough) OS, is not minimal anymore—or so I would say if I cared about systemd and all its nonsense. But I don&amp;rsquo;t see any issue with it. Yes, you&amp;rsquo;re right that systemd is more than just an init system, and with more unused code, more bugs may arise. Still, it&amp;rsquo;s the most popular init system and has been for the last 5 years or so.&lt;/p></description><content:encoded><![CDATA[<h2 id="arch-linux--btw">Arch Linux – BTW</h2>
<p>AAA, one of the most loved (cough, cough) OS, is not minimal anymore—or so I would say if I cared about systemd and all its nonsense. But I don&rsquo;t see any issue with it. Yes, you&rsquo;re right that systemd is more than just an init system, and with more unused code, more bugs may arise. Still, it&rsquo;s the most popular init system and has been for the last 5 years or so.</p>
<p>But today, we&rsquo;re not going to talk about systemd or Arch. Today is optimization day, where we optimize our Linux setup. One of the Linux distros we love to use is Arch, but what if Arch was made by the suckless team? We&rsquo;d get something like <a href="https://kisslinux.org/">KISS Linux</a>. However, for a somewhat cohesive experience, we should look at Artix Linux. You can use any init system you wish; I went with <a href="https://github.com/davmac314/dinit">dinit</a> as it is still the fastest, of course, after <a href="https://core.suckless.org/sinit/">sinit</a> and many others like <a href="https://github.com/krallin/tini">tini</a>, <a href="https://busybox.net/about.html">busybox init</a>, <a href="https://github.com/landley/toybox">toybox init</a>, and so on.</p>
<p>After a clean install with your favorite filesystem, XFS, or Btrfs for some crazy people, or even F2FS, we will continue with its optimization.</p>
<h3 id="first-basic-optimization"><strong>FIRST: Basic Optimization</strong></h3>
<p>In this section, I would like to focus on another more detailed resource: <a href="https://github.com/ventureoo/ARU">ARU</a>, the official Arch Linux guide on <a href="https://wiki.archlinux.org/title/Improving_performance">Improving Performance</a>, and the official Arch Linux boot process improvements on <a href="https://wiki.archlinux.org/title/Improving_performance/Boot_process">Improving Performance/Boot Process</a>. These are just a few examples.</p>
<p>For my own Intel Pentium N-series processor on a Dell system, I used the following <code>udev</code> rules:</p>





<pre tabindex="0"><code>/etc/udev/rules.d/60-ioschedulers.rules
# HDD
ACTION==&#34;add|change&#34;, KERNEL==&#34;sd[a-z]*&#34;, ATTR{queue/rotational}==&#34;1&#34;, ATTR{queue/scheduler}=&#34;bfq&#34;

# SSD
ACTION==&#34;add|change&#34;, KERNEL==&#34;sd[a-z]*|mmcblk[0-9]*&#34;, ATTR{queue/rotational}==&#34;0&#34;, ATTR{queue/scheduler}=&#34;bfq&#34;

# NVMe SSD
ACTION==&#34;add|change&#34;, KERNEL==&#34;nvme[0-9]*&#34;, ATTR{queue/rotational}==&#34;0&#34;, ATTR{queue/scheduler}=&#34;none&#34;</code></pre><p><strong>Setup irqbalance</strong></p>
<p>Next, we&rsquo;ll set up <a href="https://man.archlinux.org/man/irqbalance.1.en">irqbalance</a>. <code>irqbalance</code> is a daemon that distributes hardware interrupts across CPU cores to improve performance, especially on multi-core systems.</p>
<p>Let&rsquo;s install irqbalance either from the main repo or from the AUR</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">pacman -S irqbalance</span></span></code></pre></div><p>Now, just enable irqbalance, and here you go:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo systemctl <span class="nb">enable</span> irqbalance</span></span></code></pre></div><h4 id="a-zram-setup"><strong>A zram Setup</strong></h4>
<p>Now for a <code>zram</code> setup.
<a href="https://wiki.archlinux.org/title/Zram">zram</a> creates a compressed block device in RAM, which can be used for swap. This is faster than using a traditional swap partition or file, especially on systems with limited storage, though it uses RAM.</p>
<p>We will use <code>zram-generator</code> to setup <code>zram</code> thought you can use other methords like <code>udev</code> or <a href="https://aur.archlinux.org/packages/zramd/"><code>zramd</code></a></p>
<p>Let&rsquo;s install zram-generator first</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">pacman -S zram-generator</span></span></code></pre></div><p><strong>Using zram-generator</strong></p>
<p><code>zram-generator</code> provides <code>systemd-zram-setup@zramN.service</code> units to automatically initialize zram devices without users needing to enable or start the template or its instances.</p>
<p>To use it, install <code>zram-generator</code> and create <code>/etc/systemd/zram-generator.conf</code> with the following content:</p>





<pre tabindex="0"><code>[zram0]
zram-size = min(ram / 2, 4096)
compression-algorithm = zstd</code></pre><p><code>zram-size</code> is the size (in MiB) of the zram device. You can use <code>ram</code> to represent the total memory.</p>
<p><code>compression-algorithm</code> specifies the algorithm used to compress data in the zram device. Running <code>cat /sys/block/zram0/comp_algorithm</code> gives the available compression algorithms (as well as the current one included in brackets).</p>
<p>Then, run <code>daemon-reload</code> and start your configured <code>systemd-zram-setup@zramN.service</code> instance (with <code>N</code> matching the numerical instance-ID; in the example, it is <code>systemd-zram-setup@zram0.service</code>).</p>
<p>You can check the swap status of your configured <code>/dev/zramN</code> device(s) by reading the unit status of your <code>systemd-zram-setup@zramN.service</code> instance(s), using <code>zramctl</code>, or using <code>swapon</code>.</p>
<p>These are some general settings.</p>
<p>Let&rsquo;s get into Arch. First, stop <code>NetworkManager-wait-online.service</code> if you don&rsquo;t want to wait until your system get&rsquo;s network, or you cam set up <a href="https://wiki.archlinux.org/title/Iwd"><code>iwd</code></a> instead of <code>NetworkManager</code> if possible.</p>
<p>Basics only, no extra tools considered as bloat.</p>
<h4 id="configuring-pacmanconf"><strong>Configuring pacman.conf</strong></h4>





<pre tabindex="0"><code>#
# /etc/pacman.conf
#
# See the pacman.conf(5) manpage for option and repository directives

#
# GENERAL OPTIONS
#
[options]
# The following paths are commented out with their default values listed.
# If you wish to use different paths, uncomment and update the paths.
#RootDir     = /
#DBPath      = /var/lib/pacman/
#CacheDir    = /var/cache/pacman/pkg/
#LogFile     = /var/log/pacman.log
#GPGDir      = /etc/pacman.d/gnupg/
#HookDir     = /etc/pacman.d/hooks/
HoldPkg     = pacman glibc
#XferCommand = /usr/bin/curl -L -C - -f -o %o %u
#XferCommand = /usr/bin/wget --passive-ftp -c -O %o %u
#CleanMethod = KeepInstalled
Architecture = auto

# Pacman won&#39;t upgrade packages listed in IgnorePkg and members of IgnoreGroup
#IgnorePkg   =
#IgnoreGroup =

#NoUpgrade   =
#NoExtract   =

# Misc options
#UseSyslog
Color
ILoveCandy
#NoProgressBar
CheckSpace
VerbosePkgLists
ParallelDownloads = 8

# By default, pacman accepts packages signed by keys that its local keyring
# trusts (see pacman-key and its man page), as well as unsigned packages.
SigLevel    = Required DatabaseOptional
LocalFileSigLevel = Optional
#RemoteFileSigLevel = Required

# NOTE: You must run `pacman-key --init` before first using pacman; the local
# keyring can then be populated with the keys of all official Artix Linux
# packagers with `pacman-key --populate artix`.

#
# REPOSITORIES
#   - can be defined here or included from another file
#   - pacman will search repositories in the order defined here
#   - local/custom mirrors can be added here or in separate files
#   - repositories listed first will take precedence when packages
#     have identical names, regardless of version number
#   - URLs will have $repo replaced by the name of the current repo
#   - URLs will have $arch replaced by the name of the architecture
#
# Repository entries are of the format:
#       [repo-name]
#       Server = ServerName
#       Include = IncludePath
#
# The header [repo-name] is crucial - it must be present and
# uncommented to enable the repo.
#

# The gremlins repositories are disabled by default. To enable, uncomment the
# repo name header and Include lines. You can add preferred servers immediately
# after the header, and they will be used before the default mirrors.

#[system-gremlins]
#Include = /etc/pacman.d/mirrorlist

[system]
Include = /etc/pacman.d/mirrorlist

#[world-gremlins]
#Include = /etc/pacman.d/mirrorlist

[world]
Include = /etc/pacman.d/mirrorlist

#[galaxy-gremlins]
#Include = /etc/pacman.d/mirrorlist

[galaxy]
Include = /etc/pacman.d/mirrorlist

[omniverse]
Server = https://artix.sakamoto.pl/omniverse/$arch
Server = https://eu-mirror.artixlinux.org/omniverse/$arch
Server = https://omniverse.artixlinux.org/$arch

# If you want to run 32-bit applications on your x86_64 system,
# enable the lib32 repositories as required here.

#[lib32-gremlins]
#Include = /etc/pacman.d/mirrorlist

[lib32]
Include = /etc/pacman.d/mirrorlist

# custom cf kernel repo
[repo-ck]
#Server = https://mirror.lesviallon.fr/$repo/os/$arch
Server = http://repo-ck.com/$arch

# Arch
[extra]
Include = /etc/pacman.d/mirrorlist-arch

[multilib]
Include = /etc/pacman.d/mirrorlist-arch

# enabling chaotic-aur
[chaotic-aur]
Include = /etc/pacman.d/chaotic-mirrorlist

# An example of a custom package repository. See the pacman manpage for
# tips on creating your own repositories.
#[custom]
#SigLevel = Optional TrustAll
#Server = file:///home/custompkgs</code></pre><p>Okay, what was added? Custom repo like the official Arch repo. Some people believe it may cause issues, but after 2 years of using this system, I haven&rsquo;t encountered any problems.</p>
<p>Here is what was added:</p>





<pre tabindex="0"><code>Color
ILoveCandy
CheckSpace
VerbosePkgLists
ParallelDownloads = 8 # very important; configure according to your network speed</code></pre><p>Now, the <code>makepkg.conf</code>:</p>





<pre tabindex="0"><code>#!/hint/bash
#
# /etc/makepkg.conf
#

#########################################################################
# SOURCE ACQUISITION
#########################################################################
#
#-- The download utilities that makepkg should use to acquire sources
#  Format: &#39;protocol::agent&#39;
DLAGENTS=(&#39;file::/usr/bin/curl -qgC - -o %o %u&#39;
          &#39;ftp::/usr/bin/curl -qgfC - --ftp-pasv --retry 3 --retry-delay 3 -o %o %u&#39;
          &#39;http::/usr/bin/curl -qgb &#34;&#34; -fLC - --retry 3 --retry-delay 3 -o %o %u&#39;
          &#39;https::/usr/bin/curl -qgb &#34;&#34; -fLC - --retry 3 --retry-delay 3 -o %o %u&#39;
          &#39;rsync::/usr/bin/rsync --no-motd -z %u %o&#39;
          &#39;scp::/usr/bin/scp -C %u %o&#39;)

# Other common tools:
# /usr/bin/snarf
# /usr/bin/lftpget -c
# /usr/bin/wget

#-- The package required by makepkg to download VCS sources
#  Format: &#39;protocol::package&#39;
VCSCLIENTS=(&#39;bzr::breezy&#39;
            &#39;fossil::fossil&#39;
            &#39;git::git&#39;
            &#39;hg::mercurial&#39;
            &#39;svn::subversion&#39;)

#########################################################################
# ARCHITECTURE, COMPILE FLAGS
#########################################################################
#
CARCH=&#34;x86_64&#34;
CHOST=&#34;x86_64-pc-linux-gnu&#34;

#-- Compiler and Linker Flags
#CPPFLAGS=&#34;&#34;
CFLAGS=&#34;-march=native -mtune=native -O2 -pipe -fstack-protector-strong --param=ssp-buffer-size=4 -fno-plt \
        -Wp,-D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security \
        -fstack-clash-protection -fcf-protection&#34;
CXXFLAGS=&#34;${CFLAGS}&#34;
LDFLAGS=&#34;-Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now&#34;
LTOFLAGS=&#34;-flto=auto&#34;
RUSTFLAGS=&#34;-C opt-level=2 -C target-cpu=native&#34;
#-- Make Flags: change this for DistCC/SMP systems
MAKEFLAGS=&#34;-j$(getconf _NPROCESSORS_ONLN) --quiet&#34;
#-- Debugging flags
DEBUG_CFLAGS=&#34;-g&#34;
DEBUG_CXXFLAGS=&#34;$DEBUG_CFLAGS&#34;
#DEBUG_RUSTFLAGS=&#34;-C debuginfo=2&#34;

#########################################################################
# BUILD ENVIRONMENT
#########################################################################
#
# Makepkg defaults: BUILDENV=(!distcc !color !ccache check !sign)
#  A negated environment option will do the opposite of the comments below.
#
#-- distcc:   Use the Distributed C/C++/ObjC compiler
#-- color:    Colorize output messages
#-- ccache:   Use ccache to cache compilation
#-- check:    Run the check() function if present in the PKGBUILD
#-- sign:     Generate PGP signature file
#
BUILDENV=(!distcc color !ccache check !sign)
#
#-- If using DistCC, your MAKEFLAGS will also need modification. In addition,
#-- specify a space-delimited list of hosts running in the DistCC cluster.
#DISTCC_HOSTS=&#34;&#34;
#
#-- Specify a directory for package building.
BUILDDIR=/tmp/makepkg

#########################################################################
# GLOBAL PACKAGE OPTIONS
#   These are default values for the options=() settings
#########################################################################
#
# Makepkg defaults: OPTIONS=(!strip docs libtool staticlibs emptydirs !zipman !purge !debug !lto)
#  A negated option will do the opposite of the comments below.
#
#-- strip:      Strip symbols from binaries/libraries
#-- docs:       Save doc directories specified by DOC_DIRS
#-- libtool:    Leave libtool (.la) files in packages
#-- staticlibs: Leave static library (.a) files in packages
#-- emptydirs:  Leave empty directories in packages
#-- zipman:     Compress manual (man and info) pages in MAN_DIRS with gzip
#-- purge:      Remove files specified by PURGE_TARGETS
#-- debug:      Add debugging flags as specified in DEBUG_* variables
#-- lto:        Add compile flags for building with link time optimization
#
OPTIONS=(strip docs !libtool !staticlibs emptydirs zipman purge !debug !lto)

#-- File integrity checks to use. Valid: md5, sha1, sha224, sha256, sha384, sha512, b2
INTEGRITY_CHECK=(sha256)
#-- Options to be used when stripping binaries. See `man strip&#39; for details.
STRIP_BINARIES=&#34;--strip-all&#34;
#-- Options to be used when stripping shared libraries. See `man strip&#39; for details.
STRIP_SHARED=&#34;--strip-unneeded&#34;
#-- Options to be used when stripping static libraries. See `man strip&#39; for details.
STRIP_STATIC=&#34;--strip-debug&#34;
#-- Manual (man and info) directories to compress (if zipman is specified)
MAN_DIRS=({usr{,/local}{,/share},opt/*}/{man,info})
#-- Doc directories to remove (if !docs is specified)
DOC_DIRS=(usr/{,local/}{,share/}{doc,gtk-doc} opt/*/{doc,gtk-doc})
#-- Files to be removed from all packages (if purge is specified)
PURGE_TARGETS=(usr/{,share}/info/dir .packlist *.pod)
#-- Directory to store source code in for debug packages
DBGSRCDIR=&#34;/usr/src/debug&#34;

#########################################################################
# PACKAGE OUTPUT
#########################################################################
#
# Default: put built package and cached source in build directory
#
#-- Destination: specify a fixed directory where all packages will be placed
#PKGDEST=/home/packages
#-- Source cache: specify a fixed directory where source files will be cached
#SRCDEST=/home/sources
#-- Source packages: specify a fixed directory where all src packages will be placed
#SRCPKGDEST=/home/srcpackages
#-- Log files: specify a fixed directory where all log files will be placed
#LOGDEST=/home/makepkglogs
#-- Packager: name/email of the person or organization building packages
#PACKAGER=&#34;John Doe &lt;john@doe.com&gt;&#34;
#-- Specify a key to use for package signing
#GPGKEY=&#34;&#34;

#########################################################################
# COMPRESSION DEFAULTS
#########################################################################
#
COMPRESSGZ=(pigz -c -f -n)
COMPRESSBZ2=(pbzip2 -c -f)
COMPRESSXZ=(xz -T &#34;$(getconf _NPROCESSORS_ONLN)&#34; -c -z --best -)
COMPRESSZST=(zstd -c -z -q --ultra -T0 -22 -)
COMPRESSLRZ=(lrzip -9 -q)
COMPRESSLZO=(lzop -q --best)
COMPRESSZ=(compress -c -f)
COMPRESSLZ4=(lz4 -q --best)
COMPRESSLZ=(lzip -c -f)

#########################################################################
# EXTENSION DEFAULTS
#########################################################################
#
PKGEXT=&#39;.pkg.tar.zst&#39;
SRCEXT=&#39;.src.tar.gz&#39;

#########################################################################
# OTHER
#########################################################################
#
#-- Command used to run pacman as root, instead of trying sudo and su
#PACMAN_AUTH=()</code></pre><p><strong>Explanation of <code>CFLAGS</code> Options:</strong></p>
<ul>
<li><code>-march=native</code>: Optimizes the code for the specific CPU architecture of the host machine.</li>
<li><code>-mtune=native</code>: Tunes the code for the specific CPU model of the host machine.</li>
<li><code>-O2</code>: Enables a moderate level of optimization.</li>
<li><code>-pipe</code>: Uses pipes rather than temporary files for communication between different compilation stages.</li>
<li><code>-fstack-protector-strong</code>: Adds stack protection to check for buffer overflows.</li>
<li><code>--param=ssp-buffer-size=4</code>: Sets the buffer size for stack protection.</li>
<li><code>-fno-plt</code>: Avoids using the Procedure Linkage Table for function calls.</li>
<li><code>-Wp,-D_FORTIFY_SOURCE=2</code>: Enables additional security checks.</li>
<li><code>-Wformat</code>: Enables warnings for printf-like functions.</li>
<li><code>-Werror=format-security</code>: Treats format security warnings as errors.</li>
<li><code>-fstack-clash-protection</code>: Adds protection against stack clash attacks.</li>
<li><code>-fcf-protection</code>: Adds control flow protection.</li>
</ul>
<p>Yes, compression has been increased, and that is that, but it may significantly improve disk storage requirements.</p>
<p>Now, let&rsquo;s talk about the filesystem. Here, I am using Btrfs, but before that, XFS was the way to go. I still remember snapshots and other things, so Btrfs is the only option for me, until bcachefs is good. Here are my custom options; no new features were added, but IDR (I DON&rsquo;T REMEMBER).</p>
<p>Here is my <code>fstab</code>:</p>





<pre tabindex="0"><code># Static information about the filesystems.
# See fstab(5) for details.

# &lt;file system&gt; &lt;dir&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt;
# /dev/sda2 UUID=c386b909-59af-42d1-93a0-1d25fd117c87
LABEL=ROOT          	/         	btrfs     	rw,noatime,compress=zstd:5,nossd,space_cache=v2,autodefrag,commit=120,subvol=/artix/@	0 0

# /dev/sda2 UUID=c386b909-59af-42d1-93a0-1d25fd117c87
LABEL=ROOT          	/var      	btrfs     	rw,noatime,compress=zstd:5,nossd,space_cache=v2,autodefrag,commit=120,subvol=/artix/@var	0 0

# /dev/sda2 UUID=c386b909-59af-42d1-93a0-1d25fd117c87
LABEL=ROOT          	/home     	btrfs     	rw,noatime,compress=zstd:5,nossd,space_cache=v2,autodefrag,commit=120,subvol=/artix/@home	0 0

# /dev/sda2 UUID=c386b909-59af-42d1-93a0-1d25fd117c87
LABEL=ROOT          	/.snapshots	btrfs     	rw,noatime,compress=zstd:5,nossd,space_cache=v2,autodefrag,commit=120,subvol=/artix/@snapshots	0 0

# /dev/sda1 UUID=4089-8087
LABEL=EFIBOOT       	/boot     	vfat      	rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=ascii,shortname=mixed,utf8,errors=remount-ro	0 0</code></pre><p><strong>Explanation of Btrfs Options in <code>fstab</code>:</strong></p>
<ul>
<li><code>noatime</code>: Disables the updating of access times on files and directories, which can improve performance.</li>
<li><code>compress=zstd:5</code>: Enables compression using the Zstandard algorithm at level 5.</li>
<li><code>nossd</code>: Disables SSD-specific optimizations.</li>
<li><code>space_cache=v2</code>: Enables the space cache version 2, which improves performance for free space tracking.</li>
<li><code>autodefrag</code>: Automatically defragments files as they are modified.</li>
<li><code>commit=120</code>: Sets the commit interval to 120 seconds, reducing the frequency of metadata writes.</li>
</ul>
<p>Yes, I have cleanly divided into subvolumes, and there is another subvolume named <code>backup</code> which is not mounted by default. It is used to make a complete snapshot of the <code>artix</code> subvolume. This kind of <code>fstab</code> with the label is inspired by <a href="https://github.com/ChrisTitusTech/ArchTitus/">Chris Titus (archtitus)</a>.</p>
<p>And how can I forget about <a href="https://wiki.archlinux.org/title/Booster">Booster</a>, an init system? If you are not sure what I am talking about, check out this Arch Linux documentation for <a href="https://wiki.archlinux.org/title/Arch_boot_process#initramfs">initramfs</a>, which not only explains it very well but also provides you with other options to test. It&rsquo;s pretty good, but not as good as the Gentoo wiki, which is on another level.</p>
<p>Here is my <code>booster.yaml</code> config:</p>





<pre tabindex="0"><code>#universal: false
modules: btrfs,i915
compression: zstd
# zstd -9 -T0
#mount_timeout: 30s
#strip: true
#extra_files: fsck,fsck.btrfs #,vim,/usr/share/vim/vim82/
#vconsole: true</code></pre><p>It is not very complex, but it works. Here, I am not using <a href="https://github.com/Zile995/booster-um">booster-um</a> to make a custom UKI image for reasons, but the entire Booster initramfs works fine and has never caused issues like <a href="https://wiki.archlinux.org/title/Mkinitcpio">mkinitcpio</a> or <a href="https://wiki.archlinux.org/title/Dracut">dracut</a> in Arch Linux with the <a href="https://github.com/clearlinux-pkgs/linux">Clear Linux kernel</a>. I am using chaotic-aur to pull the kernel. Yes, I am not a neckbeard man waiting to compile the Linux kernel on an underpowered netbook-type laptop from the 2010s. But hey, it works. I have two kernels just as a safety measure, and I think everyone should too: my main Clear Linux kernel and the linux-lts. Before you mention it, I am on an Intel setup and have used custom kernels like <a href="https://liquorix.net/">Liquorix</a> and <a href="https://github.com/Frogging-Family/linux-tkg">tkg</a>, but they were never that very stable and did not improve performance that much. The same cannot be said about the Clear Linux kernel, which did improve my system performance by 10-12%. Not massive, but still noticeable.</p>
<p>But the kernel is not the end-all-be-all; the options passed to the kernel are just as important. Here are my custom options:</p>





<pre tabindex="0"><code>root=LABEL=ROOT rootflags=subvol=/artix/@,noatime,compress=zstd:5,nossd,space_cache=v2,autodefrag,commit=120 rw loglevel=0 console=tty2 udev.log_level=0 vt.global_cursor_default=0 mitigations=off nowatchdog msr.allow_writes=on pcie_aspm=force module.sig_unenforce intel_idle.max_cstate=1 cryptomgr.notests initcall_debug intel_iommu=igfx_off no_timer_check noreplace-smp page_alloc.shuffle=1 rcupdate.rcu_expedited=1 tsc=reliable tpm_tis.interrupts=0</code></pre><p><strong>Explanation of Kernel Options:</strong></p>
<ul>
<li><code>root=LABEL=ROOT</code>: Specifies the root filesystem by label.</li>
<li><code>rootflags=subvol=/artix/@,noatime,compress=zstd:5,nossd,space_cache=v2,autodefrag,commit=120</code>: Sets various Btrfs options for the root filesystem.</li>
<li><code>rw</code>: Mounts the root filesystem as read-write.</li>
<li><code>loglevel=0</code>: Disables kernel log messages.</li>
<li><code>console=tty2</code>: Sets the console to tty2.</li>
<li><code>udev.log_level=0</code>: Disables udev log messages.</li>
<li><code>vt.global_cursor_default=0</code>: Disables the global cursor default.</li>
<li><code>mitigations=off</code>: Disables CPU mitigations (more on this below).</li>
<li><code>nowatchdog</code>: Disables the watchdog timer.</li>
<li><code>msr.allow_writes=on</code>: Allows writes to MSR registers.</li>
<li><code>pcie_aspm=force</code>: Forces PCIe ASPM (Active State Power Management).</li>
<li><code>module.sig_unenforce</code>: Disables module signature enforcement.</li>
<li><code>intel_idle.max_cstate=1</code>: Limits the maximum C-state for Intel CPUs.</li>
<li><code>cryptomgr.notests</code>: Disables cryptographic manager tests.</li>
<li><code>initcall_debug</code>: Enables initcall debugging.</li>
<li><code>intel_iommu=igfx_off</code>: Disables Intel IOMMU for integrated graphics.</li>
<li><code>no_timer_check</code>: Disables timer checks.</li>
<li><code>noreplace-smp</code>: Disables SMP replacement.</li>
<li><code>page_alloc.shuffle=1</code>: Enables page allocation shuffling.</li>
<li><code>rcupdate.rcu_expedited=1</code>: Enables expedited RCU updates.</li>
<li><code>tsc=reliable</code>: Marks the TSC (Time Stamp Counter) as reliable.</li>
<li><code>tpm_tis.interrupts=0</code>: Disables TPM TIS interrupts.</li>
</ul>
<p>There is a controversial option, <code>mitigations=off</code>, which is very dangerous. For my old system, it hampers performance a lot, so I keep it disabled.</p>
<h3 id="more-info-on-mitigations"><strong>More Info on Mitigations:</strong></h3>
<p>Mitigations are security features introduced to protect against various CPU vulnerabilities, such as Spectre and Meltdown. Disabling mitigations can improve performance but also exposes the system to these vulnerabilities. There are two main types of mitigations:</p>
<ol>
<li><strong>Spectre Mitigations</strong>: These protect against speculative execution side-channel attacks.</li>
<li><strong>Meltdown Mitigations</strong>: These protect against out-of-order execution side-channel attacks.</li>
</ol>
<p>Disabling mitigations is generally not recommended unless you are aware of the risks and have a specific reason to do so.</p>
<p>How can we forget about <a href="https://wiki.archlinux.org/title/REFInd">rEFInd</a>, a bootloader? If you want to know more about bootloaders, refer to the Gentoo article for great detail, but the Arch wiki is also great.</p>
<p>Though a bootloader is not important and does slow down the system, as I am not using the UKI image of the kernel, we will use a bootloader. I have installed the bootloader to <code>/ESP/BOOT/</code>, which is the default place where any UEFI system looks for it.</p>
<p>Some more miscellaneous configurations:</p>
<h3 id="changing-the-journal"><strong>Changing the Journal&rsquo;s Size</strong></h3>
<p>Systemd&rsquo;s system journal&rsquo;s size can go out of control. There are some things you can do to keep it in control. If you wish to, you can also completely disable this, but I like to keep some just for emergency purposes:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">journalctl --vacuum-size<span class="o">=</span>10M
</span></span><span class="line"><span class="ln">2</span><span class="cl">journalctl --vacuum-time<span class="o">=</span>2weeks</span></span></code></pre></div><p>Never forget to install your CPU&rsquo;s microcode.</p>
<p>For Intel:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">pacman -S intel-ucode</span></span></code></pre></div><p>For AMD:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">pacman -S amd-ucode</span></span></code></pre></div><p>For other systems, I am not aware of, but you can look around. I do not believe there are any, but do quote me on that.</p>
<p>It is not all about your system. If you have slow internet or badly configured mirrors, it may cause issues on your system. Change the default mirror of Artix Linux to your specific country in the <code>/etc/pacman.d/mirrorlist</code>, and if you are on Arch or have the Arch mirror, run this:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl"><span class="nv">iso</span><span class="o">=</span><span class="k">$(</span>curl -4 ifconfig.co/country-iso<span class="k">)</span> <span class="o">&amp;&amp;</span> reflector -a <span class="m">48</span> -c <span class="nv">$iso</span> -f <span class="m">5</span> -l <span class="m">20</span> --sort rate --save /etc/pacman.d/mirrorlist</span></span></code></pre></div><p><strong>Explanation of the Command:</strong></p>
<ul>
<li><code>iso=$(curl -4 ifconfig.co/country-iso)</code>: Fetches the country ISO code using <code>curl</code>.</li>
<li><code>reflector -a 48 -c $iso -f 5 -l 20 --sort rate --save /etc/pacman.d/mirrorlist</code>: Uses <code>reflector</code> to update the mirrorlist with the fastest mirrors from the specified country.
<ul>
<li><code>-a 48</code>: Sets the age of the mirrors to 48 hours.</li>
<li><code>-c $iso</code>: Specifies the country ISO code.</li>
<li><code>-f 5</code>: Sets the number of fallback mirrors.</li>
<li><code>-l 20</code>: Limits the number of mirrors to 20.</li>
<li><code>--sort rate</code>: Sorts the mirrors by rate.</li>
<li><code>--save /etc/pacman.d/mirrorlist</code>: Saves the updated mirrorlist to the specified file.</li>
</ul>
</li>
</ul>
<h3 id="setup-swap-in-arch-linux-on"><strong>Setup Swap in Arch Linux on <code>/opt/swap</code></strong></h3>
<p>To set up a swap file on <code>/opt/swap</code>, follow these steps:</p>
<ol>
<li>Create the swap file:</li>
</ol>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo fallocate -l 2G /opt/swap</span></span></code></pre></div><ol start="2">
<li>Set the correct permissions:</li>
</ol>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo chmod <span class="m">600</span> /opt/swap</span></span></code></pre></div><ol start="3">
<li>Set up the swap space:</li>
</ol>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo mkswap /opt/swap</span></span></code></pre></div><ol start="4">
<li>Enable the swap file:</li>
</ol>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo swapon /opt/swap</span></span></code></pre></div><ol start="5">
<li>Verify that the swap file is active:</li>
</ol>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo swapon --show</span></span></code></pre></div><ol start="6">
<li>Make the change permanent by adding the following line to <code>/etc/fstab</code>:</li>
</ol>





<pre tabindex="0"><code>/opt/swap none swap sw 0 0</code></pre><h3 id="setup-swap-in-arch-linux-with-btrfs-using-the-btrfs-command-to-make-a-swap-file-at"><strong>Setup Swap in Arch Linux with Btrfs Using the Btrfs Command to Make a Swap File at <code>/opt/swaps</code></strong></h3>
<p>To set up a swap file on <code>/opt/swaps</code> using Btrfs, follow these steps:</p>
<ol>
<li>Create the swap file:</li>
</ol>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo btrfs filesystem mkswapfile --size 2G --uuid clear /opt/swaps</span></span></code></pre></div><ol start="2">
<li>Enable the swap file:</li>
</ol>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo swapon /opt/swaps</span></span></code></pre></div><ol start="3">
<li>Verify that the swap file is active:</li>
</ol>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo swapon --show</span></span></code></pre></div><ol start="4">
<li>Make the change permanent by adding the following line to <code>/etc/fstab</code>:</li>
</ol>





<pre tabindex="0"><code>/opt/swaps none swap sw 0 0</code></pre><p>For people who do not have an SSD or are not running a system from a USB flash drive, if you are, then stop using it as it may kill your USB drive faster. But what can I say? I have used a Linux install from my USB drive for 5 years, after which it failed. Your mileage may vary, but if you are using F2FS, trust me, the options in this are on another level. But if you are using a mini USB flash drive which has a very simple controller and does not have features like <a href="https://en.wikipedia.org/wiki/Wear_leveling">wear leveling</a>, then use something else. Something like <a href="https://en.wikipedia.org/wiki/YAFFS">YAFFS</a> or <a href="https://en.wikipedia.org/wiki/UBIFS">UBIFS</a> is great. Just have a look at the <a href="https://en.wikipedia.org/wiki/Flash_file_system">flash file system</a> article on Wikipedia.</p>
<p>But on topic, then use these options and commands in Btrfs will help you a lot:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">btrfs filesystem defragment -r -czstd /</span></span></code></pre></div><p><strong>Explanation of the Command:</strong></p>
<ul>
<li><code>btrfs filesystem defragment -r -czstd /</code>: Defragments the filesystem recursively with Zstandard compression.</li>
</ul>
<p>Also, add this to your <code>fstab</code>:</p>





<pre tabindex="0"><code>autodefrag</code></pre><h3 id="btrfs-scrub"><strong>Btrfs Scrub:</strong></h3>
<p>The <a href="https://btrfs.wiki.kernel.org/index.php/Glossary">Btrfs Wiki Glossary</a> says that Btrfs scrub is &ldquo;[a]n online file system checking tool. Reads all the data and metadata on the file system and uses checksums and the duplicate copies from RAID storage to identify and repair any corrupt data.&rdquo;</p>
<p>Try to run this per week to check for any errors in Btrfs:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">btrfs scrub start /</span></span></code></pre></div><h2 id="deduplication">Deduplication:</h2>
<p>Not only for HDDs, but Btrfs deduplication can also be used.</p>
<p>Using copy-on-write, Btrfs is able to copy files or whole subvolumes without actually copying the data. However, whenever a file is altered, a new proper copy is created. Deduplication takes this a step further by actively identifying blocks of data that share common sequences and combining them into an extent with the same copy-on-write semantics.</p>
<p>Tools dedicated to deduplicating a Btrfs-formatted partition include <a href="https://github.com/markfasheh/duperemove">duperemove</a> and <a href="https://github.com/Zygo/bees">bees</a>. One may also want to merely deduplicate data on a file-based level instead, using tools like <code>rmlint</code>, <code>rdfind</code>, <code>jdupes</code>, or <code>dduper</code>. For an overview of available features of those programs and additional information, have a look at the <a href="https://btrfs.wiki.kernel.org/index.php/Deduplication">upstream Wiki entry</a>.</p>
<h3 id="difference-between-bees-and-duperemove"><strong>Difference Between Bees and Duperemove:</strong></h3>
<ul>
<li>
<p><strong>Bees</strong> is a block-oriented userspace deduplication agent designed to scale up to large Btrfs filesystems. It is an offline dedupe combined with an incremental data scan capability to minimize the time data spends on disk from write to dedupe.</p>
</li>
<li>
<p><strong>Duperemove</strong> is a simple tool for finding duplicated extents and submitting them for deduplication. When given a list of files, it hashes their contents on an extent-by-extent basis and compares those hashes to each other, finding and categorizing extents that match each other. Optionally, a per-block hash can be applied for further duplication lookup. When given the <code>-d</code> option, duperemove will submit those extents for deduplication using the Linux kernel <code>FIDEDUPRANGE</code> ioctl.</p>
</li>
</ul>
<p>Duperemove can store the hashes it computes in a &lsquo;hashfile&rsquo;. If given an existing hashfile, duperemove will only compute hashes for those files that have changed since the last run. Thus, you can run duperemove repeatedly on your data as it changes without having to re-checksum unchanged data.</p>
<p>Duperemove can also take input from the <code>fdupes</code> program.</p>
<p>See the duperemove man page for further details about running duperemove.</p>
<p><strong>Deduplication:</strong></p>
<blockquote>
<p>Going by the definition in the context of filesystems, deduplication is a process of looking up identical data blocks tracked separately and creating a shared logical link while removing one of the copies of the data blocks. This leads to data space savings while increasing metadata consumption.</p></blockquote>
<p>There are two main deduplication types:</p>
<ol>
<li><strong>In-band (sometimes also called online)</strong>: All newly written data are considered for deduplication before writing.</li>
<li><strong>Out-of-band (sometimes also called offline)</strong>: Data for deduplication have to be actively looked for and deduplicated by the user application.</li>
</ol>
<p>Both have their pros and cons. Btrfs implements only the out-of-band type.</p>
<p>Btrfs provides the basic building blocks for deduplication, allowing other tools to choose the strategy and scope of the deduplication. There are multiple tools that take different approaches to deduplication, offer additional features, or make trade-offs. The following table lists tools that are known to be up-to-date, maintained, and widely used.</p>
<table>
  <thead>
      <tr>
          <th>Name</th>
          <th>File Based</th>
          <th>Block Based</th>
          <th>Incremental</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>BEES</td>
          <td>No</td>
          <td>Yes</td>
          <td>Yes</td>
      </tr>
      <tr>
          <td>duperemove</td>
          <td>Yes</td>
          <td>No</td>
          <td>Yes</td>
      </tr>
  </tbody>
</table>
<p><strong>File-Based Deduplication:</strong></p>
<p>The tool takes a list of files and tries to find duplicates among data only from these files. This is suitable, for example, for files that originated from the same base image or source of a reflinked file. Optionally, the tool could track a database of hashes and allow deduplicating blocks from more files or use that for repeated runs and update the database incrementally.</p>
<p><strong>Block-Based Deduplication:</strong></p>
<p>The tool typically scans the filesystem and builds a database of file block hashes, then finds candidate files and deduplicates the ranges. The hash database is kept as an ordinary file and can be scaled according to the needs.</p>
<p>As the files change, the hash database may get out of sync, and the scan has to be done repeatedly.</p>
<p><strong>Safety of Block Comparison:</strong></p>
<p>The deduplication inside the filesystem is implemented as an ioctl that takes a source file, destination file, and the range. The blocks from both files are compared for an exact match before merging to the same range (i.e., there’s no hash-based comparison). Pages representing the extents in memory are locked prior to deduplication and prevent concurrent modification by buffered writes or mmapped writes. Blocks are compared byte by byte and not using any hash-based approach, i.e., the existing checksums are not used.</p>
<p><strong>Limitations, Compatibility:</strong></p>
<p>Files that are subject to deduplication must have the same status regarding COW, i.e., both regular COW files with checksums, or both NOCOW, or files that are COW but don’t have checksums (NODATASUM attribute is set).</p>
<p>If the deduplication is in progress on any file in the filesystem, the send operation cannot be started as it relies on the extent layout being unchanged.</p>
<h3 id="general-settings-in-arch-linux-to-improve-performance">General Settings in Arch Linux to Improve Performance:</h3>
<ol>
<li><strong>Enable TRIM for SSDs</strong>: Ensure that TRIM is enabled for your SSD to maintain performance. You can check if TRIM is enabled with:</li>
</ol>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo fstrim -v /</span></span></code></pre></div><ol start="2">
<li><strong>Optimize Swappiness</strong>: Reduce the swappiness value to make the system less aggressive in using swap space. Edit <code>/etc/sysctl.d/99-sysctl.conf</code> and add:</li>
</ol>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">vm.swappiness<span class="o">=</span><span class="m">10</span></span></span></code></pre></div><p>Then apply the changes:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo sysctl --system</span></span></code></pre></div><ol start="3">
<li><strong>Use ZRAM</strong>: ZRAM can help improve performance by using compressed RAM as swap space. Install <code>zramswap</code>:</li>
</ol>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo pacman -S zramswap</span></span></code></pre></div><p>Then enable and start the <code>zramswap</code> service:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo systemctl <span class="nb">enable</span> zramswap
</span></span><span class="line"><span class="ln">2</span><span class="cl">sudo systemctl start zramswap</span></span></code></pre></div><ol start="4">
<li><strong>Optimize I/O Scheduler</strong>: Choose an appropriate I/O scheduler for your storage device. For SSDs, <code>noop</code> or <code>deadline</code> is recommended. For HDDs, <code>cfq</code> is a good choice. You can set the I/O scheduler with:</li>
</ol>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl"><span class="nb">echo</span> cfq <span class="p">|</span> sudo tee /sys/block/sda/queue/scheduler</span></span></code></pre></div><ol start="5">
<li>
<p><strong>Enable Lazytime Mount Option</strong>: Use the <code>lazytime</code> mount option to reduce write operations. Edit <code>/etc/fstab</code> and add <code>lazytime</code> to the mount options for your filesystems.</p>
</li>
<li>
<p><strong>Optimize Kernel Parameters</strong>: Tune kernel parameters for better performance. Edit <code>/etc/sysctl.d/99-sysctl.conf</code> and add:</p>
</li>
</ol>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">net.core.rmem_max<span class="o">=</span><span class="m">16777216</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl">net.core.wmem_max<span class="o">=</span><span class="m">16777216</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl">net.ipv4.tcp_rmem<span class="o">=</span><span class="m">4096</span> <span class="m">87380</span> <span class="m">16777216</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl">net.ipv4.tcp_wmem<span class="o">=</span><span class="m">4096</span> <span class="m">16384</span> <span class="m">16777216</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl">net.ipv4.tcp_window_scaling<span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="ln">6</span><span class="cl">net.ipv4.tcp_timestamps<span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="ln">7</span><span class="cl">net.ipv4.tcp_sack<span class="o">=</span><span class="m">1</span></span></span></code></pre></div><p>Then apply the changes:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo sysctl --system</span></span></code></pre></div><p>With some of these options, your own Artix system will be more performant—no guarantees.</p>
<p>These are just a few things, but for more performance information, please look at these resources:</p>
<ul>
<li><a href="https://www.devroom.io/2024/02/08/arch-linux-improve-boot-time-performance/">Arch Linux: Improve Boot Time Performance</a></li>
<li><a href="https://wiki.archlinux.org/title/Improving_performance">Arch Wiki: Improving Performance</a></li>
<li><a href="https://wiki.archlinux.org/title/Sysctl#Virtual_memory">Arch Wiki: Sysctl - Virtual Memory</a></li>
<li><a href="https://wiki.archlinux.org/title/Swap#Performance">Arch Wiki: Swap - Performance</a></li>
<li><a href="https://wiki.archlinux.org/title/Core_dump#Disabling_automatic_core_dumps">Arch Wiki: Core Dump - Disabling Automatic Core Dumps</a></li>
<li><a href="https://wiki.archlinux.org/title/Improving_performance/Boot_process">Arch Wiki: Improving Performance/Boot Process</a></li>
<li><a href="https://www.reddit.com/r/archlinux/comments/rz6294/arch_linux_laptop_optimization_guide_for">Reddit: Arch Linux Laptop Optimization Guide</a></li>
<li><a href="https://gist.github.com/dante-robinson/cd620c7283a6cc1fcdd97b2d139b72fa">GitHub Gist: Arch Linux Optimization</a></li>
</ul>
]]></content:encoded></item><item><title>The Best Compression Algo Maybe</title><link>http://localhost:1313/blog/the-best-compression-algo-maybe/</link><pubDate>Wed, 09 Oct 2024 00:00:00 +0000</pubDate><author>jayeshjoshi08jj@gmail.com (hkcfs)</author><guid>http://localhost:1313/blog/the-best-compression-algo-maybe/</guid><description>&lt;h2 id="compression-algorithms-the-unsung-heroes-of-the-internet">&lt;strong>Compression Algorithms: The Unsung Heroes of the Internet&lt;/strong>&lt;/h2>
&lt;p>Compression algorithms are the silent workhorses that made the internet boom of the 1990s possible. They enabled pirates to download gigabytes of data squeezed into mere megabytes, albeit with a tradeoff: higher electricity bills during decompression or the patience to wait as files unraveled byte by byte.&lt;/p>
&lt;p>In modern times, compression is often overlooked. Storage is dirt cheap, and bandwidth feels limitless for most of us. Yet, compression remains invaluable, particularly in data centers and archival zones where storage efficiency takes precedence over speed. It’s no coincidence that popular filesystems like Btrfs, NTFS, and ZFS have built-in compression features. These tools may take a few extra seconds during operation, but they can save organizations thousands of dollars in hard drives or SSDs—a worthy trade-off if you ask me.&lt;/p></description><content:encoded><![CDATA[<h2 id="compression-algorithms-the-unsung-heroes-of-the-internet"><strong>Compression Algorithms: The Unsung Heroes of the Internet</strong></h2>
<p>Compression algorithms are the silent workhorses that made the internet boom of the 1990s possible. They enabled pirates to download gigabytes of data squeezed into mere megabytes, albeit with a tradeoff: higher electricity bills during decompression or the patience to wait as files unraveled byte by byte.</p>
<p>In modern times, compression is often overlooked. Storage is dirt cheap, and bandwidth feels limitless for most of us. Yet, compression remains invaluable, particularly in data centers and archival zones where storage efficiency takes precedence over speed. It’s no coincidence that popular filesystems like Btrfs, NTFS, and ZFS have built-in compression features. These tools may take a few extra seconds during operation, but they can save organizations thousands of dollars in hard drives or SSDs—a worthy trade-off if you ask me.</p>
<p>But the story today is different. We’ve moved past the era of efficient, practical compression. Now, the pursuit is maximum compression, no matter how long it takes. Shrinking gigabytes of data into a few tens of megabytes is the name of the game. Enter <strong>cmix</strong>.</p>
<hr>
<h2 id="compression-today-maximum-over-practicality"><strong>Compression Today: Maximum Over Practicality</strong></h2>
<p>Today’s story is different. We’re less interested in efficient, fast compression and more focused on achieving <em>maximum</em> compression ratios. Who cares if it takes seven days to compress a file, as long as we shrink gigabytes of data into a few tens of megabytes? That’s where <strong>cmix</strong> comes in.</p>
<p><a href="https://www.byronknoll.com/cmix.html">cmix</a> is a lossless data compression program designed to achieve exceptional compression ratios at the cost of extreme CPU and memory usage. It consistently achieves state-of-the-art results, as seen in benchmarks like the <a href="https://www.mattmahoney.net/dc/text.html">Large Text Compression Benchmark</a> by Matt Mahoney. cmix earned second place, just behind <strong>nncp v3.2</strong>.</p>
<p>Here’s the trade-off: cmix is <em>not</em> fast or memory efficient. For instance, in the benchmark:</p>
<ul>
<li><strong>nncp v3.2</strong> (1st place) took 241,871 seconds and used 7.6 GB of RAM.</li>
<li><strong>cmix</strong> (2nd place) took 622,949 seconds and consumed 30.95 GB of RAM.</li>
</ul>
<p>While cmix isn’t practical for day-to-day use, it’s an exciting tool for anyone exploring extreme compression.</p>
<hr>
<h2 id="setting-up-cmix"><strong>Setting Up cmix</strong></h2>
<p>Installing cmix is straightforward, though some adjustments may be needed depending on your system.</p>
<h3 id="hardware-and-os-specs"><strong>Hardware and OS Specs</strong></h3>
<p>For testing, I used the following setup:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">OS: Ubuntu 22.04.5 LTS x86_64
</span></span><span class="line"><span class="ln">2</span><span class="cl">Host: Google Compute Engine
</span></span><span class="line"><span class="ln">3</span><span class="cl">Kernel: 6.1.91-060191-generic
</span></span><span class="line"><span class="ln">4</span><span class="cl">CPU: AMD EPYC 7B13 <span class="o">(</span>16<span class="o">)</span> @ 2.449GHz
</span></span><span class="line"><span class="ln">5</span><span class="cl">Memory: 37054MiB / 64297MiB</span></span></code></pre></div><h3 id="installation-steps"><strong>Installation Steps</strong></h3>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo apt update
</span></span><span class="line"><span class="ln">2</span><span class="cl">sudo apt install git
</span></span><span class="line"><span class="ln">3</span><span class="cl">git clone https://github.com/byronknoll/cmix
</span></span><span class="line"><span class="ln">4</span><span class="cl"><span class="nb">cd</span> cmix
</span></span><span class="line"><span class="ln">5</span><span class="cl">make</span></span></code></pre></div><p>If you’re lucky, this will work flawlessly. If not, here’s what I had to do to get it running on Ubuntu 22.04.5 LTS.</p>
<ol>
<li>Install dependencies:





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">sudo apt install clang libc++-dev</span></span></code></pre></div></li>
<li>Modify the <code>Makefile</code>:





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-diff" data-lang="diff"><span class="line"><span class="ln">1</span><span class="cl"><span class="gd">- CC = clang++-17
</span></span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="gd"></span><span class="gi">+ CC = clang++
</span></span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="gi"></span><span class="gd">- all: LFLAGS += -Ofast -march=native
</span></span></span><span class="line"><span class="ln">4</span><span class="cl"><span class="gd"></span><span class="gi">+ all: LFLAGS += -O3 -ffast-math -march=native
</span></span></span></code></pre></div></li>
<li>Recompile:





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">make</span></span></code></pre></div></li>
</ol>
<p>If everything works, you can run <code>./cmix --help</code> which should give you:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln"> 1</span><span class="cl">cmix version <span class="m">21</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl">Compress:
</span></span><span class="line"><span class="ln"> 3</span><span class="cl">    with dictionary:    cmix -c <span class="o">[</span>dictionary<span class="o">]</span> <span class="o">[</span>input<span class="o">]</span> <span class="o">[</span>output<span class="o">]</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl">    without dictionary: cmix -c <span class="o">[</span>input<span class="o">]</span> <span class="o">[</span>output<span class="o">]</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">    force text-mode:    cmix -t <span class="o">[</span>dictionary<span class="o">]</span> <span class="o">[</span>input<span class="o">]</span> <span class="o">[</span>output<span class="o">]</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl">    no preprocessing:   cmix -n <span class="o">[</span>input<span class="o">]</span> <span class="o">[</span>output<span class="o">]</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">    only preprocessing: cmix -s <span class="o">[</span>dictionary<span class="o">]</span> <span class="o">[</span>input<span class="o">]</span> <span class="o">[</span>output<span class="o">]</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">                        cmix -s <span class="o">[</span>input<span class="o">]</span> <span class="o">[</span>output<span class="o">]</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">Decompress:
</span></span><span class="line"><span class="ln">10</span><span class="cl">    with dictionary:    cmix -d <span class="o">[</span>dictionary<span class="o">]</span> <span class="o">[</span>input<span class="o">]</span> <span class="o">[</span>output<span class="o">]</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl">    without dictionary: cmix -d <span class="o">[</span>input<span class="o">]</span> <span class="o">[</span>output<span class="o">]</span></span></span></code></pre></div><hr>
<h2 id="testing-cmix-bee-movie-script"><strong>Testing cmix: Bee Movie Script</strong></h2>
<p>To test cmix, I used the <em>Bee Movie</em> script. Here are the results:</p>
<h3 id="original-file"><strong>Original File</strong></h3>
<ul>
<li>File Size: <strong>86,091 bytes</strong> (~85 KB)</li>
</ul>
<h3 id="compressed-file-cmix"><strong>Compressed File (cmix)</strong></h3>
<ul>
<li>File Size: <strong>21,966 bytes</strong> (~22 KB)</li>
<li>Time Taken: <strong>124.65 seconds</strong></li>
</ul>
<p><img src="/images/btop-bee-movie.jpg" alt="System resource usage displayed using btop, showing one CPU core usage at 100%, 13.9GB of RAM used by the “cmix” process, and an average CPU usage of 25% for that process for compressing the BEE movie Script."></p>
<p>Command:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln">1</span><span class="cl">./cmix -c bee-movie.txt bee-movie.txt.cmix
</span></span><span class="line"><span class="ln">2</span><span class="cl">Detected block types: TEXT: 100.0%
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="m">86091</span> bytes -&gt; <span class="m">21966</span> bytes in 124.65 s.
</span></span><span class="line"><span class="ln">4</span><span class="cl">cross entropy: 2.041
</span></span><span class="line"><span class="ln">5</span><span class="cl">
</span></span><span class="line"><span class="ln">6</span><span class="cl">it took 124.65 s.</span></span></code></pre></div><p>Repeated trials brought the time down slightly (108.77 seconds), but it’s still not “fast.” For comparison, I tested other algorithms:</p>
<table>
  <thead>
      <tr>
          <th><strong>Algorithm</strong></th>
          <th><strong>Compressed Size (bytes)</strong></th>
          <th><strong>Time Taken</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>gzip</td>
          <td>33,765</td>
          <td>~0.5s</td>
      </tr>
      <tr>
          <td>xz</td>
          <td>31,100</td>
          <td>~0.5s</td>
      </tr>
      <tr>
          <td>zstd (level 22)</td>
          <td>31,639</td>
          <td>~0.5s</td>
      </tr>
      <tr>
          <td>zpaq</td>
          <td>25,195</td>
          <td>~0.8s</td>
      </tr>
      <tr>
          <td><strong>cmix</strong></td>
          <td><strong>21,966</strong></td>
          <td><strong>124.65s</strong></td>
      </tr>
  </tbody>
</table>
<p>While cmix achieves the best compression, its time and resource demands are immense.</p>
<hr>
<h2 id="compressing-the-linux-kernel"><strong>Compressing the Linux Kernel</strong></h2>
<p>For a tougher test, I attempted to compress the original Linux 1.0 source code (released on March 13, 1994). Here are the numbers:</p>
<table>
  <thead>
      <tr>
          <th><strong>File</strong></th>
          <th><strong>Size (bytes)</strong></th>
          <th><strong>Time Taken</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Original</td>
          <td>5171200 (5.0M)</td>
          <td>-</td>
      </tr>
      <tr>
          <td>gzip</td>
          <td>1259175 (1.3M)</td>
          <td>~0.308s</td>
      </tr>
      <tr>
          <td>xz</td>
          <td>935892 (914K)</td>
          <td>~2.895s</td>
      </tr>
      <tr>
          <td>zstd</td>
          <td>697924 (682K)</td>
          <td>~16.73s</td>
      </tr>
      <tr>
          <td><strong>cmix</strong></td>
          <td><strong>516406 (505K)</strong></td>
          <td><strong>7441.04s (131m34.256s)</strong></td>
      </tr>
  </tbody>
</table>
<p><img src="/images/btop-linux-kernel.jpg" alt="The system load shown using btop having cpu at around 60% load, with 16 GB of ram used up by cmix with an average cpu usage of 25% while compressing the linux 1.0 Kernel"></p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="nb">time</span> ./cmix -c ../linux-1.0.tar ../linux-1.0.tar.cmix
</span></span><span class="line"><span class="ln"> 2</span><span class="cl">Detected block types: TEXT: 91.3% DEFAULT: 8.7%
</span></span><span class="line"><span class="ln"> 3</span><span class="cl">progress: 12.32%
</span></span><span class="line"><span class="ln"> 4</span><span class="cl">progress: 20.16%
</span></span><span class="line"><span class="ln"> 5</span><span class="cl"><span class="m">5171200</span> bytes -&gt; <span class="m">516406</span> bytes in 7441.04 s.
</span></span><span class="line"><span class="ln"> 6</span><span class="cl">cross entropy: 0.799
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">real    131m34.256s
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">user    121m12.508s
</span></span><span class="line"><span class="ln">10</span><span class="cl">sys     2m52.326s</span></span></code></pre></div><p>I initially planned to test the Linux 6.12.3 tarball (1.54 GB), but cmix proved impractical, taking over 9 minutes to reach just 0.01% progress. Even older versions like Linux 2.5 took too long. This highlights cmix’s limitations with large files.</p>
<hr>
<h2 id="final-thoughts"><strong>Final Thoughts</strong></h2>
<p>Despite its impracticality for everyday use, cmix holds a special place in my heart. It was the first program I compiled while working on my project, <strong>osqr</strong>, and exploring its potential was both fun and enlightening. Compression may feel like a relic of the past, but tools like cmix remind us of the art and science behind squeezing data to its absolute minimum.</p>
<p>If nothing else, cmix is a testament to how far we can push the limits of lossless compression. It may not be practical, but it’s a lot of fun.</p>
<p>What are your thoughts on cmix and other compression algorithms? Let me know below!</p>
]]></content:encoded></item><item><title>BTRFS - the Best FileSystem</title><link>http://localhost:1313/blog/btrfs-the-best-filesystem/</link><pubDate>Tue, 30 Jul 2024 00:00:00 +0000</pubDate><author>jayeshjoshi08jj@gmail.com (hkcfs)</author><guid>http://localhost:1313/blog/btrfs-the-best-filesystem/</guid><description>&lt;p>B.T.R.F.S., B-tree FS, Butter FS, or whatever you want to call it, I would argue, is one of the best filesystems, at least for me.&lt;/p>
&lt;p>BTRFS is a filesystem just like any other, but its features compel users to try it. This is what has made me a lover of it. There is a reason why Facebook (Meta) uses it, and Oracle used to develop it.&lt;/p>
&lt;p>One of the best things I love about BTRFS is snapshots and compression, which I live by. No longer are the days where updating Arch (~btw) feels like terror about to strike or a new package or massive update (e.g., KDE5 to KDE6) can kill or break your system.&lt;/p></description><content:encoded><![CDATA[<p>B.T.R.F.S., B-tree FS, Butter FS, or whatever you want to call it, I would argue, is one of the best filesystems, at least for me.</p>
<p>BTRFS is a filesystem just like any other, but its features compel users to try it. This is what has made me a lover of it. There is a reason why Facebook (Meta) uses it, and Oracle used to develop it.</p>
<p>One of the best things I love about BTRFS is snapshots and compression, which I live by. No longer are the days where updating Arch (~btw) feels like terror about to strike or a new package or massive update (e.g., KDE5 to KDE6) can kill or break your system.</p>
<p>I have an old Dell laptop with 4GB RAM and an Intel Pentium N-something—not powerful. It cannot even play unmodded Minecraft at 30 FPS or any powerful game. It does not have an HDD because, when I opened it for the very first time (it was my first laptop, and I had never opened a laptop or fixed a computer before), I broke the cable. The HDD was fine, but that garbage cable is so damn weak. I could just blow air on it, and it would break. So, I installed Arch Linux on a 16GB spare USB thumb drive, which—even if it breaks—it does not matter, as it’s a small USB drive:</p>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="ln">1</span><span class="cl">rw, noatime, compress=zstd:5, nossd, space_cache=v2, autodefrag, commit=120, subvolid=302, subvol=~~somethings~~</span></span></code></pre></div><p>These are my options on the HDD, which I primarily used for data storage. I have been using it for the last two years, and it has served me well since then.</p>
<p>The thumb drive runs F2FS with compression and some other optimizations, which I took for AI. It runs quite well, and my minimal install of Arch only uses ~4GB compressed with multiple browsers and LabWC—all the comfort I want.</p>
]]></content:encoded></item><item><title>Arch Linux - BTW</title><link>http://localhost:1313/blog/arch-linux-btw/</link><pubDate>Sun, 21 Jul 2024 00:00:00 +0000</pubDate><author>jayeshjoshi08jj@gmail.com (hkcfs)</author><guid>http://localhost:1313/blog/arch-linux-btw/</guid><description>&lt;blockquote>
&lt;p>&amp;ldquo;I have been using Arch Linux for the longest time, and it has been a great system—or at least, that’s what I would say in a perfect world.&amp;rdquo;&lt;/p>&lt;/blockquote>
&lt;p>Arch Linux is like LFS (Linux From Scratch) but simpler. Since the introduction of &lt;code>archinstall&lt;/code>, it has become more&amp;hellip; let’s just say &amp;ldquo;dumber,&amp;rdquo; though it’s still quite a good system.&lt;/p>
&lt;p>If you want to learn about a Linux system but don’t want to waste 60+ hours of your life on Gentoo before binaries, it’s quite a good distro. However, I wouldn’t recommend using the &lt;code>archinstall&lt;/code> system. What’s the point if you do? I’d only recommend Arch if you know what you’re doing or if you’ve manually installed it at least twice with custom settings, like using BTRFS, custom repositories, or even Artix instead of Arch.&lt;/p></description><content:encoded><![CDATA[<blockquote>
<p>&ldquo;I have been using Arch Linux for the longest time, and it has been a great system—or at least, that’s what I would say in a perfect world.&rdquo;</p></blockquote>
<p>Arch Linux is like LFS (Linux From Scratch) but simpler. Since the introduction of <code>archinstall</code>, it has become more&hellip; let’s just say &ldquo;dumber,&rdquo; though it’s still quite a good system.</p>
<p>If you want to learn about a Linux system but don’t want to waste 60+ hours of your life on Gentoo before binaries, it’s quite a good distro. However, I wouldn’t recommend using the <code>archinstall</code> system. What’s the point if you do? I’d only recommend Arch if you know what you’re doing or if you’ve manually installed it at least twice with custom settings, like using BTRFS, custom repositories, or even Artix instead of Arch.</p>
<p>Arch is a rolling release, which means it doesn’t have defined versions like most software. It just keeps updating—and will do so as long as it’s maintained.</p>
<p>That said, it comes with its own set of challenges. If you want to do something unique, like running Linux from Google Drive (<a href="https://ersei.net/en/blog/fuse-root">source</a>), Arch Linux can handle it. Though, I would argue Alpine Linux is better for such cases.</p>
<p>All in all, Arch is a flexible, minimal, and community-driven Linux distro. People love it, and so do I. It has its challenges, but there’s a reason why I keep switching back to it.</p>
]]></content:encoded></item></channel></rss>